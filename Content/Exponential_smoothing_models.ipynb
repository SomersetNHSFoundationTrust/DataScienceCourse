{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports for workbook\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_squared_error,\n",
    ")\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt, ETSModel\n",
    "\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking forecast models\n",
    "\n",
    "\n",
    "def s_naive(ts, period, forecast_horizon):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        ts (pandas.Series): Historical time series observations (in order)\n",
    "        period (int): Seasonal period (i.e., 12 for monthly data)\n",
    "        forecast_horizon (int): Number of timesteps forecasted into the future\n",
    "    Outputs:\n",
    "        list: Forecasted time series\n",
    "    \"\"\"\n",
    "\n",
    "    most_recent_seasonal = ts[-period:].to_list()\n",
    "\n",
    "    # We now need to make the forecast\n",
    "    # Number of times to multiply the list to ensure we meet forecast horizon\n",
    "    mult_list = int(np.ceil(forecast_horizon / period))\n",
    "\n",
    "    return (most_recent_seasonal * mult_list)[:forecast_horizon]\n",
    "\n",
    "\n",
    "def drift_method(ts, forecast_horizon):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        ts (pandas.Series): Historical time series observations (in order)\n",
    "        forecast_horizon (int): Number of timesteps forecasted into the future\n",
    "    Outputs:\n",
    "        list: Forecasted time series\n",
    "    \"\"\"\n",
    "\n",
    "    latest_obs = ts.iloc[-1]\n",
    "    first_obs = ts.iloc[0]\n",
    "\n",
    "    slope = (latest_obs - first_obs) / (len(ts) - 1)\n",
    "\n",
    "    forecast_list = [latest_obs + slope * h for h in range(1, forecast_horizon + 1)]\n",
    "\n",
    "    return forecast_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we will read in monthly A&E attendances from NHS England (following the same steps as the first two worksheets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import raw data\n",
    "raw_data = pd.read_excel(\n",
    "    \"https://www.england.nhs.uk/statistics/wp-content/uploads/sites/2/2024/02/Monthly-AE-Time-Series-January-2024.xls\",\n",
    "    skiprows=13,\n",
    "    sheet_name=\"Activity\",\n",
    ")\n",
    "# Make calender adjustments\n",
    "raw_data[\"Period\"] = pd.to_datetime(raw_data[\"Period\"], format=\"%Y-%m-%d\")\n",
    "raw_data = raw_data[[\"Period\", \"Total Attendances\"]].set_index(\"Period\").asfreq(\"MS\")\n",
    "# Add columns showing number of days for given month\n",
    "raw_data[\"days_per_month\"] = raw_data.index.daysinmonth\n",
    "# Add column for adjusted data\n",
    "raw_data[\"avg_daily_attendances\"] = (\n",
    "    raw_data[\"Total Attendances\"] / raw_data[\"days_per_month\"]\n",
    ")\n",
    "# We will again initially consider data pre-COVID.\n",
    "ts_data = raw_data.loc[:\"2020-02-01\"]  # limit data to Feb 2020\n",
    "post_covid = raw_data.loc[\"2020-02-01\":]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time series of data using Plotly express line\n",
    "ts_fig = px.line(\n",
    "    ts_data,\n",
    "    x=ts_data.index,\n",
    "    y=\"avg_daily_attendances\",\n",
    "    title=\"Monthly A&E attendances from NHS England\",\n",
    ")\n",
    "ts_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential smoothing (along with ARIMA) is one of the most widely used techniques in time series forecasting. \n",
    "\n",
    "The exponential smoothing methods use weighted averages of past observations (where weights decay exponentially for older values). In other words, the forecast is more influenced by recent values, compared with older observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Exponential Smoothing (SES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y}_{T + 1 | T} = \\alpha y_{T} + \\alpha(1-\\alpha) y_{T-1} + \\alpha(1-\\alpha)^2 y_{T-2} + ...$, where $0\\leq \\alpha \\leq1$ is the smoothing parameter. \n",
    "\n",
    "Note, $\\hat{y}_{T + 1 | T}$ is our 1-step forecast based on data $y_{1} + y_{2} + ... + y_{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponentially decaying weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much weight do we want to give to the most recent observation? This is what alpha controls.\n",
    "\n",
    "alpha = 0.1  # (alpha close to 1 ~ naive)\n",
    "weight_list = []\n",
    "\n",
    "for step in range(len(ts_data)):\n",
    "    weighting = alpha * ((1 - alpha) ** step)\n",
    "    weight_list.append(weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 5 weights\n",
    "weight_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decaying weights\n",
    "weighting_fig = px.bar(y=weight_list, title=f\"Weights decay with alpha = {alpha}\")\n",
    "weighting_fig.update_layout(xaxis_title=\"Timestep\", yaxis_title=\"Weight\")\n",
    "weighting_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a dataframe of the attendances and the corresponding weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SES_data = ts_data[[\"avg_daily_attendances\"]].copy()\n",
    "SES_data[\"weighting\"] = weight_list[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SES_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our forecast, we would simply multiply each weight with the corresponding observation (starting with the highest weight for the most recent observation) and sum the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate forecast\n",
    "ses_forecast = (SES_data[\"avg_daily_attendances\"] * SES_data[\"weighting\"]).sum()\n",
    "print(ses_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_fig = go.Figure()\n",
    "fcst_fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=ts_data.index, y=ts_data[\"avg_daily_attendances\"], name=\"Historical data\"\n",
    "    )\n",
    ")\n",
    "fcst_fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=post_covid.index, y=[ses_forecast] * len(post_covid), name=\"SES forecast\"\n",
    "    )\n",
    ")\n",
    "fcst_fig.update_layout(\n",
    "    yaxis_title=\"avg_daily_attendances\",\n",
    "    title=\"Simple exponential smoothing (weights approach)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method of SES can be flawed in certain circumstances. Remember, when calculating the mean, each weight has a value $\\dfrac{1}{len(data)}$. We don't adjust the weighting to ensure it sums to 1 in SES, so need to check this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check weights sum to 1\n",
    "np.sum(weight_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SES using an iterative approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y}_{T + 1 | T} = \\alpha y_{T}+(1-\\alpha)\\hat{y}_{T | T - 1}$<br>$\\hat{y}_{1} = y_{1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run through an example of using this iterative approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SES_data = ts_data[[\"avg_daily_attendances\"]].copy()\n",
    "SES_data[\"Forecast\"] = np.nan\n",
    "SES_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y}_{1} = y_{1}$ so we will use the first observation for our first forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SES_data.loc[pd.to_datetime(\"2010-08-01\"), [\"Forecast\"]] = SES_data[\n",
    "    \"avg_daily_attendances\"\n",
    "].iloc[0]\n",
    "SES_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets calculate $\\hat{y}_{2 | 1}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SES_data.loc[pd.to_datetime(\"2010-09-01\"), [\"Forecast\"]] = (\n",
    "    alpha * SES_data[\"avg_daily_attendances\"].iloc[1]\n",
    "    + (1 - alpha) * SES_data[\"Forecast\"].iloc[0]\n",
    ")\n",
    "SES_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, forecast $\\hat{y}_{3 | 2}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SES_data.loc[pd.to_datetime(\"2010-10-01\"), [\"Forecast\"]] = (\n",
    "    alpha * SES_data[\"avg_daily_attendances\"].iloc[2]\n",
    "    + (1 - alpha) * SES_data[\"Forecast\"].iloc[1]\n",
    ")\n",
    "SES_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would take a lot of cells to run through the entire dataset...its probably more efficient to do this in a loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a series and alpha, return series of smoothed points\n",
    "def SES_iterator(obs_series, alpha):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        obs_series (pandas.Series): Observational data.\n",
    "        alpha (float): 0<=alpha<=1, the smoothing parameter.\n",
    "    Output:\n",
    "        list: Fitted values (one step forecasts in training data)\n",
    "    \"\"\"\n",
    "\n",
    "    # First fitted value (we'll use our first observation for this)\n",
    "    ses_fit = [obs_series.iloc[0]]\n",
    "\n",
    "    # Loop through time series\n",
    "    for step in range(1, len(obs_series)):\n",
    "\n",
    "        # Make a 1-step forecast\n",
    "        one_step_fcst = obs_series.iloc[step] * alpha + (1 - alpha) * ses_fit[step - 1]\n",
    "        # Add forecast to list\n",
    "        ses_fit.append(one_step_fcst)\n",
    "\n",
    "    return ses_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run iterative SES model\n",
    "ses_list = SES_iterator(SES_data[\"avg_daily_attendances\"], alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check first three are the same as our manual calculation\n",
    "ses_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The forecast will simply be the last value (flat forecast)\n",
    "ses_list[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty much identical to our previous approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print forecast when using weights approach\n",
    "print(ses_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, with large T, this will converge even closer to our original forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the fitted model and the forecast from our iterative approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_fig = go.Figure()\n",
    "fcst_fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=ts_data.index, y=ts_data[\"avg_daily_attendances\"], name=\"Historical data\"\n",
    "    )\n",
    ")\n",
    "fcst_fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=SES_data.index, y=ses_list, name=\"Fitted SES model (iterative approach)\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fcst_fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=post_covid.index,\n",
    "        y=[ses_list[-1]] * len(post_covid),\n",
    "        name=\"SES forecast (latest fitted value)\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fcst_fig.update_layout(\n",
    "    title=\"Simple exponential smoothing (iterative approach)\",\n",
    "    yaxis_title=\"avg_daily_attendances\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite a lot of work for a simple forecast. Luckily, there is a much more efficient way of doing this... `statsmodels`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SES using statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our attendance data to SimpleExpSmoothing from statsmodels\n",
    "ses_model = SimpleExpSmoothing(ts_data[\"avg_daily_attendances\"]).fit(\n",
    "    smoothing_level=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets calculate a foreast from our fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We supply the start and end date for our forecast (which we've done for post COVID period here)\n",
    "ses_series = ses_model.predict(start=post_covid.index[1], end=post_covid.index[-1])\n",
    "ses_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_fig = go.Figure()\n",
    "fcst_fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=ts_data.index, y=ts_data[\"avg_daily_attendances\"], name=\"Historical data\"\n",
    "    )\n",
    ")\n",
    "fcst_fig.add_trace(\n",
    "    go.Scatter(x=ses_series.index, y=ses_series, name=\"SES forecast (statsmodels)\")\n",
    ")\n",
    "fcst_fig.update_layout(\n",
    "    title=\"Simple exponential smoothing (statsforecast)\",\n",
    "    yaxis_title=\"avg_daily_attendances\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Simple Exponential Smoothing key points</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simple and intuitive algorithm that uses exponentially decaying weights to form an average (which you control with $\\alpha$).\n",
    "* The forecasts will be flat.\n",
    "* Useful forecast model if your data has **no trend or seasonal patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1 (15-20 minutes)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will look at the daily A&E admissions dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating some fake time series data\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "time_index = pd.date_range(start=\"2022-01-01\", end=\"2024-05-01\", freq=\"D\")\n",
    "\n",
    "seasonal_component = (10 * np.sin((2 * np.pi / 7) * np.arange(len(time_index)))).astype(\n",
    "    int\n",
    ")\n",
    "remainder_component = np.random.normal(loc=0, scale=1.5, size=len(time_index)).astype(\n",
    "    int\n",
    ")\n",
    "trend_component = np.round(np.arange(len(time_index)) / 35).astype(int)\n",
    "\n",
    "time_series = 50 + seasonal_component + trend_component + remainder_component\n",
    "\n",
    "# Format data in a pandas dataframe\n",
    "task_data = pd.DataFrame({\"date\": time_index, \"admissions_daily\": time_series})\n",
    "task_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = task_data[: (-4 * 7)]\n",
    "testing_data = task_data[(-4 * 7) :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fit and forecast a simple exponential smoothing model using statsmodels on the daily A&E `training_data` dataset (set `smoothing_level` to 0.2). Look at the statsmodels documentation if you're unsure on anything, available [here](https://www.statsmodels.org/devel/examples/notebooks/generated/exponential_smoothing.html). Create a new column in `testing_data` to store the forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How does the forecast look? Use Plotly (or another visualisation library of your choice) to plot forecasts against the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How does changing `alpha` imapct output forecasts? Add 3 additional forecasts with `smoothing_level` values of  0.01, 0.5, 0.9 (store each as a new column in the `testing_data`) to the forecast figure, to compare the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Exponential Smoothing (Holt's linear trend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous forecast gave a flat forecast for the weighted average. If our time series if showing a clear trend, how can we include a trend in the forecast?\n",
    "\n",
    "Holt's linear trend is an extension of SES, where we now have two smoothing equations. One for the level ($\\alpha$) and one for the trend ($\\beta$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecast equation: $\\hat{y}_{t + h | t} = l_{t} + hb_{t}$\n",
    "\n",
    "Level equation: $l_{t} = \\alpha y_{t} + (1 - \\alpha)(l_{t-1} + b_{t-1})$<br>\n",
    "Trend equation: $b_{t} = \\beta(l_{t} - l_{t-1}) + (1 - \\beta)b_{t-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Holt's method from scratch (feel free to ignore this)\n",
    "\n",
    "\n",
    "def holts_linear_method(time_series, alpha, beta):\n",
    "\n",
    "    level_component = []\n",
    "    trend_component = []\n",
    "    fitted_model = []\n",
    "\n",
    "    for t, y in enumerate(time_series):\n",
    "\n",
    "        if t == 0:\n",
    "            # **We ned to first estimate our starting values!**\n",
    "            # Estimate first level.\n",
    "            level = y\n",
    "            # Estimate first trend.\n",
    "            trend = time_series[1] - y\n",
    "\n",
    "        else:\n",
    "            # Now we can start the iteration process!\n",
    "            level = alpha * y + (1 - alpha) * (\n",
    "                level_component[-1] + trend_component[-1]\n",
    "            )\n",
    "            trend = (\n",
    "                beta * (level - level_component[-1]) + (1 - beta) * trend_component[-1]\n",
    "            )\n",
    "\n",
    "        # Combine components to create a 1-step forecast\n",
    "        forecast = level + trend\n",
    "\n",
    "        level_component.append(level)\n",
    "        trend_component.append(trend)\n",
    "        fitted_model.append(forecast)\n",
    "\n",
    "    return level_component, trend_component, fitted_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract manual calculation of Holts method components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_level_component, manual_trend_component, manual_forecast = holts_linear_method(\n",
    "    ts_data[\"avg_daily_attendances\"], 0.1, 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using statsmodels certaintly requires less code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holt_model = Holt(endog=ts_data[\"avg_daily_attendances\"]).fit(\n",
    "    smoothing_level=0.1, smoothing_trend=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the trend and level componants look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=2, subplot_titles=(\"Level component\", \"Trend component\")\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=holt_model.level.index, y=holt_model.level.values, name=\"Level (statsmodels)\"\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=holt_model.trend.index, y=holt_model.trend.values, name=\"Trend (statsmodels)\"\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=holt_model.level.index,\n",
    "        y=manual_level_component,\n",
    "        name=\"Level (manual calculation)\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=holt_model.trend.index,\n",
    "        y=manual_trend_component,\n",
    "        name=\"Trend (manual calculation)\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=350, width=900, title_text=\"Componants of Holt's Linear Trend Method\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`statsforecast` is probably a better option than calculating each component from scratch... lets make forecasts from our `holt_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holt_series = holt_model.predict(start=post_covid.index[1], end=post_covid.index[-1])\n",
    "holt_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualise Holt's linear trend forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holt_fcst_fig = px.line(x=holt_series.index, y=holt_series)\n",
    "holt_fcst_fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=ts_data.index, y=ts_data[\"avg_daily_attendances\"], name=\"Historical data\"\n",
    "    )\n",
    ")\n",
    "holt_fcst_fig.update_layout(\n",
    "    yaxis_title=\"avg_daily_attendances\", xaxis_title=None, title=\"Holt's linear method\"\n",
    ")\n",
    "holt_fcst_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Damped trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear trend may not always be the best forecast (it may be unreasonable to expect constant linear growth). We can adjust our model to include a damped trend simply by passing `damped_trend=True`. We won't go into detail on this method, however, we introduce a new damping parameter, $\\phi$, where $0 \\leq \\phi \\leq 1$ (see [fpp](https://otexts.com/fpp2/holt.html) for more info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holt_model_damped = Holt(endog=ts_data[\"avg_daily_attendances\"], damped_trend=True).fit(\n",
    "    smoothing_level=0.1, smoothing_trend=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict attendances during post-COVID period\n",
    "holts_damped = holt_model_damped.predict(\n",
    "    start=post_covid.index[1], end=post_covid.index[-1]\n",
    ")\n",
    "holts_damped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holt_fcst_fig.add_trace(\n",
    "    go.Scatter(x=holts_damped.index, y=holts_damped, name=\"Damped trend\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Double Exponential Smoothing key points</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extension of simple exponential smoothing\n",
    "* Suitable if you time series has clear trend, ***but no seasonality.***\n",
    "* Introduces a new smoothing parameter ($\\beta$) which allows you to adjust the weighting for the trend component.\n",
    "* Can also use a damped trend if linear trend is not expected (may be useful for long forecast horizons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 (10-15 mins)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use `Holt` to make a linear trend forecast trained on the `training_data` dataset (use `smoothing_level`= 0.01 & `smoothing_trend`= 0.01 )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Plot the linear trend forecast alongside the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triple Exponential Smoothing (Holt-Winters seasonal method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we consider a model that can incoorperate both seasonal and trend components. Holt-Winters extendes Holt's method, by adding a third smoothing equation for seasonality.\n",
    "\n",
    "We therefore have three smoothing parameters, $\\alpha$ (for level), $\\beta$ (for trend), and $\\gamma$ (for seasonality).\n",
    "\n",
    "Forecast equation: $\\hat{y}_{t + h | t} = l_{t} + hb_{t} + s_{t+h-m(k+1)}$, where k is the integer part from $\\frac{h-1}{m}$.\n",
    "\n",
    "Level equation: $l_{t} = \\alpha (y_{t} - s_{t-m}) + (1 - \\alpha)(l_{t-1} + b_{t-1})$<br>\n",
    "Trend equation: $b_{t} = \\beta(l_{t} - l_{t-1}) + (1 - \\beta)b_{t-1}$<br>\n",
    "Seaonal equation: $s_{t} = \\gamma(y_{t} - l_{t-1} - b_{t-1}) + (1 - \\gamma)s_{t-m}$\n",
    "\n",
    "We can also have a model for multiplicative seasonality, but we will just consider the additive model here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code gets a bit messier for triple exponential smoothing, so we won't derive each component from scratch, as we did for previous models. This would, however, be a great exercise to understand the process in more depth if its something that interests you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use `statsmodels` to fit a Holt Winters model to our attendance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_model = ExponentialSmoothing(\n",
    "    endog=ts_data[\"avg_daily_attendances\"],\n",
    "    seasonal_periods=12,\n",
    "    trend=\"add\",\n",
    "    seasonal=\"add\",\n",
    ").fit(smoothing_level=0.6, smoothing_trend=0.1, smoothing_seasonal=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=3,\n",
    "    subplot_titles=(\"Level componant\", \"Trend componant\", \"Seasonal componant\"),\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=hw_model.level.index, y=hw_model.level.values, name=\"Level\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=hw_model.trend.index, y=hw_model.trend.values, name=\"Trend\"),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=hw_model.season.index, y=hw_model.season.values, name=\"Season\"),\n",
    "    row=1,\n",
    "    col=3,\n",
    ")\n",
    "\n",
    "fig.update_layout(height=350, width=900, title_text=\"Componants of Holt-Winters model\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict attendances during post-COVID period\n",
    "hw_series = hw_model.predict(start=post_covid.index[1], end=post_covid.index[-1])\n",
    "hw_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Holt-Winters forecast\n",
    "hw_series_fig = px.line(x=hw_series.index, y=hw_series)\n",
    "hw_series_fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=ts_data.index, y=ts_data[\"avg_daily_attendances\"], name=\"Historical data\"\n",
    "    )\n",
    ")\n",
    "hw_series_fig.update_layout(\n",
    "    yaxis_title=\"avg_daily_attendances\", xaxis_title=None, title=\"Holt-Winters method\"\n",
    ")\n",
    "hw_series_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our forecasts are starting to look good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One major challange is estimating optimal values for $\\alpha$, $\\beta$,and $\\gamma$. Visualising the forecast or trial and error probably isnt the best approach to find the optimal values...\n",
    "\n",
    "There are a number of techniques we could use (such as cross-validation and train-test splits), but luckily there is a framework that can automate this selection process for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ExponenTialSmoothing (ETS) framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ExponenTialSmoothing (ETS) framework provides automated selection of parameters ($\\alpha$, $\\beta$,and $\\gamma$) and also allows you to extract probabalistic information from the model (i.e., a distribution of likely outcomes). Rather than providing a point forecast as we've previously done ETS returns prediction intervals!\n",
    "\n",
    "It's vital to use prediciton intervals, as there is no other way to show the level of certainty in a single point-forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting our data using `ETSModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ets_model = ETSModel(\n",
    "    endog=ts_data[\"avg_daily_attendances\"],\n",
    "    # All we need to do is specify the seasonal period,\n",
    "    # And whether the trend/seasonal components are additive or multiplicative\n",
    "    seasonal_periods=12,\n",
    "    trend=\"add\",\n",
    "    seasonal=\"add\",\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict from trained model\n",
    "ets_series = ets_model.predict(start=post_covid.index[1], end=post_covid.index[-1])\n",
    "ets_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_series_fig = px.line(x=ets_series.index, y=ets_series)\n",
    "hw_series_fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=ts_data.index, y=ts_data[\"avg_daily_attendances\"], name=\"Historical data\"\n",
    "    )\n",
    ")\n",
    "hw_series_fig.update_layout(\n",
    "    yaxis_title=\"avg_daily_attendances\", xaxis_title=None, title=\"ETS modelling\"\n",
    ")\n",
    "hw_series_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the parameters the model has selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ets_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to extract uncertainty is to create multiple simulated forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = ets_model.simulate(nsimulations=47, anchor=\"end\", repetitions=25)\n",
    "sims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise all simulations\n",
    "hw_series_fig = px.line(x=ets_series.index, y=ets_series)\n",
    "\n",
    "hw_series_fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=ts_data.index, y=ts_data[\"avg_daily_attendances\"], name=\"Historical data\"\n",
    "    )\n",
    ")\n",
    "\n",
    "for sim_num, col in enumerate(sims.columns):\n",
    "    hw_series_fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=sims.index,\n",
    "            y=sims[col].values,\n",
    "            line=dict(color=\"rgba(105, 105, 105, 0.1)\"),\n",
    "            name=f\"sim_num ={sim_num}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "hw_series_fig.update_layout(\n",
    "    yaxis_title=\"avg_daily_attendances\",\n",
    "    xaxis_title=None,\n",
    "    title=\"ETS modelling (using simulation)\",\n",
    ")\n",
    "hw_series_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could take quantiles of these simulations (i.e., 2.5th and 97.5th for 95% prediction intervals) or get them directly from statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = ets_model.get_prediction(\n",
    "    start=\"2020-03-01\", end=raw_data.index[-1]\n",
    ").summary_frame(alpha=0.025)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_series_fig = px.line(x=pred_df.index, y=pred_df[\"mean\"])\n",
    "\n",
    "hw_series_fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=ts_data.index, y=ts_data[\"avg_daily_attendances\"], name=\"Historical data\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "hw_series_fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=pred_df.index,\n",
    "        y=pred_df[\"pi_lower\"].values,\n",
    "        line=dict(color=\"rgba(105, 105, 105, 0.5)\"),\n",
    "        name=\"yhat_lower\",\n",
    "    )\n",
    ")\n",
    "hw_series_fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=pred_df.index,\n",
    "        y=pred_df[\"pi_upper\"].values,\n",
    "        line=dict(color=\"rgba(105, 105, 105, 0.5)\"),\n",
    "        name=\"yhat_upper\",\n",
    "    )\n",
    ")\n",
    "\n",
    "hw_series_fig.update_layout(\n",
    "    yaxis_title=\"avg_daily_attendances\",\n",
    "    xaxis_title=None,\n",
    "    title=\"ETS modelling (prediction intervals)\",\n",
    ")\n",
    "hw_series_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3 (20-30 mins)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use statsmodels `ETSModel` to train a ETS model using `training_data` and automatically estimate the best exponential smoothing model (set `trend` and `seasonal` both to `'add'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Obtain the prediction, along with 95% prediction intervals, using the `get_prediction()` method, and plot the results. Use the code above if you're unsure on this, or read the [documentation](https://www.statsmodels.org/devel/generated/statsmodels.tsa.exponential_smoothing.ets.ETSModel.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a dataframe that stores the testing data, the ETS forecast and any two benchmarking forecasts (all trained on the training data). How does the performance of ETS compare with the benchmarking models? Use MAPE (or another metric of your choice) to summarise model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
