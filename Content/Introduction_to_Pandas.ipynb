{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5c9e11d1202222",
   "metadata": {},
   "source": [
    "# Introduction to Pandas\n",
    "\n",
    "Knowledge of Pandas is a must-have for anyone wishing to use Python for data analysis! Pandas is a powerful open-source library that provides new data structures, such as the Series and the DataFrame that allows you to efficiently handle relational data. Over the next few cells, you'll discover the key features of Pandas. This notebook is designed to provide you with the basics of Pandas, before we look at a case study with real data in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96d867debb3070d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T06:55:55.997189600Z",
     "start_time": "2024-04-16T06:55:55.480705900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd # We normally import pandas with the alias 'pd'\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea07f2b2ba6d600",
   "metadata": {},
   "source": [
    "### Recap Task\n",
    "\n",
    "Create a function that will take in a tuple containing a shape and returns a random array.\n",
    "E.g., if I pass (5, 5) I should end up with a 5x5 array of random numbers.\n",
    "\n",
    "Then use your function to create a 10x10 array and save it as a variable named `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd19489a60226d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T06:55:56.014115300Z",
     "start_time": "2024-04-16T06:55:55.997189600Z"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dcee0932e20381",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "Now we have some data, we can create a DataFrame. This is ordinarily done using a `pd.read_` command, e.g., `pd.read_sql` takes in connection information to a database and a sql query and will return a dataframe containing the results. We'll look more at `read_` later today, but for now, we'll use the random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eec7f07d45b73a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T06:56:03.047451700Z",
     "start_time": "2024-04-16T06:56:02.997322500Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can turn the data into a DataFrame\n",
    "df = pd.DataFrame(data) # df is a standard variable name for a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9319dcd53d91858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can take a look at the dataframe by just typing df\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4b2c07aa7f4469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or we can look at the top n rows (default 5)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd296cd2f93b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or the final n rows (default 5)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763141a4ef44533",
   "metadata": {},
   "source": [
    "Notice that the columns are 'labelled' as 0 - 9. This is because when we created the DataFrame we didn't provide any labels for the columns, we could reload it with column names, or rename the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911ba2ba47264ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({0:\"ColumnA\", 1:\"ColumnB\"}, axis=1) # will rename the first 2 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbffcf30ab61853",
   "metadata": {},
   "source": [
    "### NOTE:\n",
    "\n",
    "This does not save the changes made to the `DataFrame`, lots of commands in Pandas will produce a new `DataFrame` with the changes in, rather than make them in place (it's always worth checking the behaviour though). \n",
    "\n",
    "We could change that by providing the keyword argument `inplace=True` for some functions, or we could save over the variable `df`, e.g., \n",
    "\n",
    "`df = df.rename({0:\"ColumnA\", 1:\"ColumnB\"}, axis=1)`\n",
    "\n",
    "but for now, we'll leave it with the default numeric columns.\n",
    "\n",
    "## Summary Statistics\n",
    "\n",
    "We can also get some summary statistics from the `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51955300a68fe64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae3526e37af864b",
   "metadata": {},
   "source": [
    "This works well with numeric data, but let's try with some nominal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a60c996af8cef1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T06:56:28.746699400Z",
     "start_time": "2024-04-16T06:56:28.720341100Z"
    }
   },
   "outputs": [],
   "source": [
    "categories = np.random.choice([\"red\", \"blue\", \"green\"], 10) # create a random column containing red, green and blue.\n",
    "\n",
    "categories = pd.Series(categories, name=\"colours\") # turn it into a Pandas series with the label 'colours'\n",
    "\n",
    "print(categories) # take a look at the new column\n",
    "\n",
    "df = pd.concat([df, categories], axis=1) # Creates a new DataFrame with the new column added on the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e56e54fb91341",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Take a look at the new `DataFrame` to check that the new column is there, then generate the summary statistics, what do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fcb7a30dec6508",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T06:56:54.863111500Z",
     "start_time": "2024-04-16T06:56:54.827Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561fa108415e3588",
   "metadata": {},
   "source": [
    "Let's take a look at the datatypes in the `DataFrame` to try and understand this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd3d4a29508195",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aed72b9ca9fdb5d",
   "metadata": {},
   "source": [
    "You should be able to see that the first 10 columns are of type `float64` and the final type is an `object`. Pandas tries to infer the datatype from the data if it's not explicitly given.\n",
    "\n",
    "## Selecting Data\n",
    "\n",
    "Let's take a closer look at the final 2 columns. There are a few ways to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f43f5c86c0c615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, -2:] # iloc allows you to reference [rows, columns] by their index, e.g., here we want all the rows, and from the second to last column onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0aa0ac78b9d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, [9, \"colours\"]] # loc allows you to select columns by name [rows, [column_names]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f458b9cc8321f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[9, \"colours\"]] # If you want all rows, you can also just specify the columns [[column_names]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be82baf7ea26e9a6",
   "metadata": {},
   "source": [
    "`iloc` is the least preferred way to select data, can you think of a reason why?\n",
    "\n",
    "We can use `loc` or column selection to add columns too, if we reference a column that does not yet exist, and set values to it, it will actually create a column in the dataframe!\n",
    "\n",
    "We can also convert data types, for categorical data, pandas has a special dtype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5487b82a8f4a9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"colours\"] = df.loc[:, \"colours\"].astype(\"category\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71edfc1e06d9321",
   "metadata": {},
   "source": [
    "Behind the scenes, this is actually using a code instead of the categories for processing, we can access these codes via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01010bf66b86282",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"colours\"].cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b2f79e7ce4292c",
   "metadata": {},
   "source": [
    "Or if we wanted to see a map between the codes and the categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4af13dfad6d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary by enumerating what every is returned from df[\"colours\"].cat.categories (which will be the categories)\n",
    "dict(enumerate(df[\"colours\"].cat.categories)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e100e77e6434ca",
   "metadata": {},
   "source": [
    "Using the final 2 columns, let's see if we can calculate the average value per colour, so in pseudocode we want to:\n",
    "\n",
    "- Select the last 2 columns\n",
    "- group by the colour\n",
    "- calculate the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65e85f67a5c599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going through the pseudocode:\n",
    "#  - Select the last 2 columns, we know these are names 9 and \"colours\" and that we want all rows: df.loc[:, [9, \"colours]]\n",
    "#  - group by the colour: pandas has a groupby method for this exact thing!\n",
    "#  - calculate the mean: An object output from groupby needs more information to specify what to do with the groups, \n",
    "#    e.g., we could sum them, in this case, we just want mean().\n",
    "\n",
    "df.loc[:, [9, \"colours\"]].groupby(\"colours\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69f026c63b77495",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Adapt the above approach to calculate the mean values per colour for 2 columns in a single `DataFrame`.\n",
    "\n",
    "Check the `DataFrame` looks how you expect it to look, then save it as the variable `grouped_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce6a41aa625c309",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T08:24:33.457653700Z",
     "start_time": "2024-04-16T08:24:33.436851400Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b61538e5dd95588",
   "metadata": {},
   "source": [
    "## Basic Plotting\n",
    "\n",
    "It is possible to generate quick plots using Pandas. The plotting in Pandas is built on top of the common Python plotting library `matplotlib` library, so it is possible to customise these plots, but better approaches to plotting will be covered in forthcoming session. For now, the purpose of this is to get a feel for the data quickly. \n",
    "\n",
    "The basic format to plotting is:\n",
    "\n",
    "`df.plot()` for a line graph, or\n",
    "`df.plot.OTHERGRAPH` where `OTHERGRAPH` could be box, bar, kde etc... (see the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) for an extensive list).\n",
    "\n",
    "We'll use a bar plot to compare our average column values for each colour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879701b5817cda19",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94477371349916b",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Using the same 2 columns you selected above, create a boxplot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e8ba462316e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T07:33:38.956939700Z",
     "start_time": "2024-04-16T07:33:38.925707300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325ce25719ba1f2",
   "metadata": {},
   "source": [
    "## Pivot and Crosstab\n",
    "\n",
    "We've already seen the `groupby` keyword, but there are other ways to manipulate a `DataFrame`, some of the most common ones are `pivot` and `crosstab`. `pivot` acts exactly like a pivot table in Excel and allows you to choose columns to be the index and columns, and aggregates values together.\n",
    "\n",
    "### Task\n",
    "\n",
    "Start by adding a new column to `df` with a random choice of `\"A\"` or `\"B\"` and call it `\"alpha/beta\"`.\n",
    "\n",
    "For bonus marks, can you save the new column with a categorical data type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf315a59220db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "df.loc[:, \"alpha/beta\"] = np.random.choice([\"A\", \"B\"], 10)\n",
    "df[\"alpha/beta\"] = df[\"alpha/beta\"].astype(\"category\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b5bd0267bcc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should display a 12 column DataFrame with 10 random float64 columns, a column labelled \"colour\" and a column labelled \"alpha/beta\".\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0d36cc9bb6ecb6",
   "metadata": {},
   "source": [
    "We can use our 2 new columns to calculate the mean value in any other column, for instance column 4, for each combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b650400579ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(df, index = \"colours\", columns = \"alpha/beta\", values = 4, aggfunc = \"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd1a98c5daf339b",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "You may see `NaN` in our `pivot_table`, this stands for Not A Number, and means there is a missing value, e.g., if green & B is `NaN` there are no values associated when `colours` is green and `alpha/beta` is B.\n",
    "\n",
    "### Task\n",
    "\n",
    "Make a pivot table out of a different column, and use a different aggregation function.\n",
    "\n",
    "HINT: If you're unsure about the options available, check the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb37a7a47cc0175d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T07:33:49.040238700Z",
     "start_time": "2024-04-16T07:33:49.022097100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed21a14b0b3043bd",
   "metadata": {},
   "source": [
    "`crosstab`s are a special case of `pivot_table` that return the count of 2 (or more) variables. You can achieve something very similar with `aggfunc = 'count'` in a `pivot_table`.\n",
    "\n",
    "Notice any 0s in the `DataFrame` below, these will be located in the same places as the `NaN` values above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261066f72589bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index = df[\"colours\"], columns = df[\"alpha/beta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3257240033cbddd",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "\n",
    "Let's suppose that some of our data was missing from the original `DataFrame`, say ~10% of our numeric data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c3cb284ce93e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_data = df.copy()\n",
    "\n",
    "indices = np.random.choice([*range(10)], (10, 2))\n",
    "\n",
    "for x, y in indices:\n",
    "    df_missing_data.iloc[x, y] = np.nan\n",
    "\n",
    "df_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6567f6823c59e83",
   "metadata": {},
   "source": [
    "First, let's calculate how much of our data is missing, this won't equal exactly 10% because we're not including the qualitative columns from the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10ba2a6ca2475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Missing data: {df_missing_data.isna().values.sum() / df_missing_data.size * 100 :.2f}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db4f6ad61666b3",
   "metadata": {},
   "source": [
    "Breaking down the above:\n",
    "\n",
    "- `f\"`: Specifies that this is an 'f-string', which is a string that can contain code encapsulated in curly brackets ({}).\n",
    "- `df_missing_data.isna()`: This returns a `DataFrame` containing `True` and `False`, where the `True` values are in the place of any missing data.\n",
    "- `.values`: Retrieves the underlying NumPy array.\n",
    "- `.sum()`: This is a neat little trick in Python, where `True` and `False` is the same as 1 and 0 respectively, so by adding up all the `True` (or 1) values, we get the total number of missing values.\n",
    "- `df_missing_data.size`: Returns the number of elements in the `DataFrame`.\n",
    "- `* 100`: Hopefully by now this is self-explanatory!\n",
    "- `:.2f`: An f-string formatting trick, we're stating that we only want to display 2 decimal places for our #float."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2658cc4b5654ca6",
   "metadata": {},
   "source": [
    "Machine learning algorithms are often very sensitive to missing data, so we need to try and make a 'best-guess' to fill in the data, you will learn more robust algorithms to do this in the future.\n",
    "\n",
    "We have a few options here, perhaps we want to fill in the missing data with the mean per column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18175f57ddca39ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = df_missing_data.select_dtypes(np.float64).mean(axis=0)\n",
    "\n",
    "df_means_filled = df_missing_data.fillna(means, axis=0) \n",
    "df_means_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17e8da0535b9f86",
   "metadata": {},
   "source": [
    "Perhaps we want to use our qualitative columns to provide some insight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4fdbd515acc3e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T07:33:57.039079600Z",
     "start_time": "2024-04-16T07:33:56.989908700Z"
    }
   },
   "outputs": [],
   "source": [
    "df_category_filled = df_missing_data.copy()\n",
    "\n",
    "colour_alpha_beta_means = df.groupby(\n",
    "    [\"colours\", \"alpha/beta\"]\n",
    ").mean().mean(axis=1).to_dict()\n",
    "\n",
    "for row_index, col_index in zip(*np.where(df_category_filled.isna())):\n",
    "    value = colour_alpha_beta_means[\n",
    "        tuple(\n",
    "            df_category_filled.loc[row_index, [\"colours\", \"alpha/beta\"]].values\n",
    "        )\n",
    "    ]\n",
    "    df_category_filled.iloc[row_index, col_index] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ece864cf54234",
   "metadata": {},
   "source": [
    "For this, we can easily see which imputation method fit better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21ea77e76fbdaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(imputed_df, original_df, columns):\n",
    "    \"\"\"\n",
    "    Calculate the Root Mean Square Error (RMSE) between imputed and original DataFrame for specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "        imputed_df (DataFrame): DataFrame with imputed values.\n",
    "        original_df (DataFrame): Original DataFrame with true values.\n",
    "        columns (List[str]): List of column names to calculate RMSE for.\n",
    "    \n",
    "    Returns:\n",
    "        float: Root Mean Square Error (RMSE) value.\n",
    "    \"\"\"\n",
    "    diff = imputed_df.loc[:, columns] - original_df.loc[:, columns]\n",
    "    \n",
    "    rmse = np.sqrt(np.mean(diff**2))\n",
    "    return rmse\n",
    "    \n",
    "\n",
    "print(f\"RMSE when using a category fill imputation: {calculate_rmse(df_category_filled, df, [*range(10)]) :.3f}\")\n",
    "print(f\"RMSE when using a means fill imputation: {calculate_rmse(df_means_filled, df, [*range(10)]) :.3f}\")\n",
    "\n",
    "# In this case, it's likely to be very similar! You'll look at better imputation methods in a forthcoming section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9ac0eb",
   "metadata": {},
   "source": [
    "## Combining Datasets\n",
    "\n",
    "Combining datasets can be done in a variety of ways, we can do a vertical mash together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataframe that's similar to the first, feel free to print it out and have a look at it!\n",
    "new_df = pd.DataFrame(\n",
    "    zip(\n",
    "        *np.random.rand(10, 10), \n",
    "        np.random.choice([\"red\", \"blue\", \"green\"], size=10), \n",
    "        np.random.choice([\"alpha\", \"beta\"], size=10)\n",
    "    )\n",
    ").rename({10:\"colours\", 11:\"alpha/beta\"}, axis=1)\n",
    "\n",
    "\n",
    "# concat sticks the new dataframe on the bottom of the first, notice the indices in the following\n",
    "pd.concat([df, new_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cce505",
   "metadata": {},
   "source": [
    "We can do a horizontal mash together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae05f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new dataframe containing 2 columns\n",
    "extra_cols = pd.DataFrame(\n",
    "    zip(\n",
    "        np.random.choice([\"yes\", \"no\"], size=10), \n",
    "        np.random.choice([\"left\", \"both\", \"right\"], size=10, p=[0.45, 0.1, 0.45])\n",
    "    ), columns = [\"yes/no\", \"dominant_hand\"]\n",
    ")\n",
    "\n",
    "# Squashes them together horizontally\n",
    "pd.concat([df, extra_cols], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dec005",
   "metadata": {},
   "source": [
    "We can also `merge` together (this is similar the behaviour in SQL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e31c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code does a few things, see if you can work out what's going on, feel free to pull it apart to test each bit!\n",
    "new_df = df.groupby(\n",
    "    [\"colours\", \"alpha/beta\"]\n",
    ").mean().reset_index().dropna()[[\"colours\", \"alpha/beta\", 9]].rename({9:10}, axis=1)\n",
    "\n",
    "# Look out for the colour purple in the \"colours\" column!\n",
    "new_df.loc[len(df.index)] = (\"purple\", \"A\", 1)\n",
    "\n",
    "# Inner joins merge dataframes together with keys that appear in both datasets\n",
    "pd.merge(left=df, right=new_df, how=\"inner\") # so no purple value should appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cbb868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also specify a left join, where we use keys that only appear in the left dataframe\n",
    "pd.merge(left=df, right=new_df, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc738162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, we can also specify a right join, where we use keys that only appear in the right dataframe\n",
    "pd.merge(left=df, right=new_df, how=\"right\") # That purple value should appear now! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e40770c",
   "metadata": {},
   "source": [
    "### TASK\n",
    "\n",
    "Read through the types of merges in pandas (https://pandas.pydata.org/docs/reference/api/pandas.merge.html#) and try a outer and cross."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d756d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdeef0d91ac9630",
   "metadata": {},
   "source": [
    "## Time series data\n",
    "\n",
    "Let's say the data we had was actually taken on certain days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b7b6b49fda200",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_months = 10\n",
    "years_months_days = {\"year\": [2023]*number_of_months, \"month\": [*range(1, number_of_months+1)], \"day\": [1]*number_of_months}\n",
    "\n",
    "date_df = pd.DataFrame(years_months_days)\n",
    "display(date_df) \n",
    "#  we can convert this into a single column:\n",
    "dates = pd.to_datetime(date_df)\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c243df564d0a16e",
   "metadata": {},
   "source": [
    "### TASK\n",
    "\n",
    "Add the new column to your dataframe with the column label of \"dates\" and verify the datatype is what you're expecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d35774aabbbda24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T07:35:20.972581900Z",
     "start_time": "2024-04-16T07:35:20.932560700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc28c15968336abe",
   "metadata": {},
   "source": [
    "Pandas has lots of utility functions to deal with time series data, let's create some random data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18720a3fe083d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range(start='1/1/2023', end='31/1/2023')\n",
    "random_numbers = np.random.rand(dates.shape[0])\n",
    "\n",
    "# notice that we set the dates to the index, this doesn't have to be done at creation, \n",
    "# but makes things easier if we want to take advantage of pandas time series utilities.\n",
    "sample_time_df = pd.DataFrame(index = dates, data=random_numbers)\n",
    "sample_time_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13f0c9fee7d5ab3",
   "metadata": {},
   "source": [
    "We'll go through 4 main functions that make working with time series data really easy in pandas.\n",
    "\n",
    "1) `shift`: we can move our values forwards or backwards using shift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a22411a587481",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_time_df.shift(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfef61ef6d30b84",
   "metadata": {},
   "source": [
    "2) `resample`: The values can be resampled using a different time frequency, e.g. weekly, and give you values as of the start of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec398df4994d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_time_df.resample('W').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ee0b4f50ecfa0",
   "metadata": {},
   "source": [
    "3) `asfreq`: returns the value at the end of the time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa4fe47f561c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_time_df.asfreq('W')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6a0fb3b62da0e7",
   "metadata": {},
   "source": [
    "4) `rolling`: returns a rolling window that can be used with an aggregation function (e.g., mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3991168ecb66c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_time_df.rolling(3).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86af4099a63a90f8",
   "metadata": {},
   "source": [
    "### TASK\n",
    "\n",
    "1) Can you create a plot that contains all of your numeric columns\n",
    "2) Under that plot, can you also plot the rolling total of the numeric values (TIP: start by writing the pseudocode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b006d965b0da656",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T13:04:04.842255Z",
     "start_time": "2024-04-02T13:04:04.832461400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8054a089fac0f26",
   "metadata": {},
   "source": [
    "## Apply and map\n",
    "\n",
    "If we want to apply a function to each row in a dataframe we could iterate through the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7fb5ffa06c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_one = lambda row: row + 1\n",
    "[add_one(row) for row in df[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178d0177ce01bc6e",
   "metadata": {},
   "source": [
    "But pandas also provides the `apply` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7d4838c5d9780",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0].apply(add_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316bbe09813c407",
   "metadata": {},
   "source": [
    "We can apply functions to entire dataframes too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde9646faa94fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(float).apply(add_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac66402a965a53",
   "metadata": {},
   "source": [
    "Similarly, if we want to map a value to something, we can use a dictionary and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227ea45e861ef7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {\"A\": \"alpha\", \"B\":\"beta\"}\n",
    "\n",
    "[mapping_dict[row] for row in df[\"alpha/beta\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d007f85b70f9c85",
   "metadata": {},
   "source": [
    "Or we can use the `map` functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b24b42368938e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"alpha/beta\"].map(mapping_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4603b537e54b9109",
   "metadata": {},
   "source": [
    "### TASK\n",
    "\n",
    "Update the colours column of the  dataframe, `df`, to use the first letter of each colour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2d403aafbf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315332cb96dc3e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you're finished, run this code. It should be error free if you have done the task successfully.\n",
    "assert not (set(df[\"colours\"]) - {\"r\", \"g\", \"b\"}), \"Looks like you have values other than 'r', 'g', 'b' in your dataframe!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aca74761d9aa67",
   "metadata": {},
   "source": [
    "Congratulations on getting this far, this concludes the pandas introduction! This afternoon you'll be putting this all into practice on a real dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
