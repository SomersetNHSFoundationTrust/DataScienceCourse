{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Time Series Forecasting"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports required for notebook\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from plotly import graph_objects as go\n",
    "\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excellent freely available resource on time series forecasting**\n",
    "\n",
    "https://otexts.com/fpp3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is time series data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series is a series of data points indexed in time order. Typically, time series is a sequence of points taken as successive, equally spaced points in time. More information on time series data is available [here](https://en.wikipedia.org/wiki/Time_series)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In healthcare, we may record time series data for:\n",
    "* Hospital admissions and discharges (i.e., daily admissions).\n",
    "* Ward beds (i.e., bed occupancy recorded at midnight).\n",
    "* Patient vital signs (i.e., hourly temperature recording)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating some fake example time series data\n",
    "time_index = pd.date_range(start=\"2022-01-01\", end=\"2024-05-01\", freq=\"W\")\n",
    "\n",
    "time_series_example = pd.DataFrame(index=time_index)\n",
    "time_series_example[\"Referrals\"] = [np.random.poisson(160) for x in time_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 5 rows\n",
    "time_series_example.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line chart for example time series\n",
    "px.line(\n",
    "    time_series_example,\n",
    "    x=time_series_example.index,\n",
    "    y=\"Referrals\",\n",
    "    title=\"Example monthly time series dataset for referrals\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with time series in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at monthly A&E admissions across England (data available [here](https://www.england.nhs.uk/statistics/statistical-work-areas/ae-waiting-times-and-activity/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data used in notebook\n",
    "raw_data = pd.read_excel(\n",
    "    \"https://www.england.nhs.uk/statistics/wp-content/uploads/sites/2/2024/02/Monthly-AE-Time-Series-January-2024.xls\",\n",
    "    skiprows=13,\n",
    "    sheet_name=\"Activity\",\n",
    ")\n",
    "\n",
    "raw_data[\"Period\"] = pd.to_datetime(raw_data[\"Period\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "raw_data = raw_data[[\"Period\", \"Total Attendances\"]].set_index(\"Period\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first few rows of data\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line chart for A&E monthly admissions\n",
    "px.line(\n",
    "    raw_data,\n",
    "    x=raw_data.index,\n",
    "    y=\"Total Attendances\",\n",
    "    title=\"NHS England A&E attendances monthly time series\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post COVID data gets quite messy! For ease, lets only analyse data pre-COVID. Of course, we can't just ignore the impact of COVID on data, but this is something we will address in the optional time series session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data post-COVID\n",
    "raw_data = raw_data.loc[:\"2020-02-01\"]\n",
    "raw_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise pre-COVID attendance data\n",
    "px.line(\n",
    "    raw_data,\n",
    "    x=raw_data.index,\n",
    "    y=\"Total Attendances\",\n",
    "    title=\"NHS England A&E attendances monthly time series\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are seeing a pattern of repeated variation in this data (i.e., monthly seasonality). That is, variations that occur in the data at specific intervals (i.e., daily, monthly or yearly).\n",
    "\n",
    "\n",
    "One factor that could be influencing seasonal variation when looking at total monthly attendances, is simply the number of days in each month. ***We should remove calendar variation before starting the analysis.*** We can instead calculate average daily attendances per month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calendar adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract useful information from `pandas.datetime`  objects. A few examples are shown below, but for a full list of attributes, look [here](https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DatetimeIndex.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get day of week\n",
    "raw_data.index.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get month\n",
    "raw_data.index.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get days in month\n",
    "raw_data.index.daysinmonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with the number of days for a given month\n",
    "raw_data[\"days_per_month\"] = raw_data.index.daysinmonth\n",
    "\n",
    "# Show the new column in table\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make calender adjustment\n",
    "raw_data[\"avg_daily_attendances\"] = (\n",
    "    raw_data[\"Total Attendances\"] / raw_data[\"days_per_month\"]\n",
    ")\n",
    "\n",
    "# Show adjusted data\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot adjusted data using a Plotly line chart\n",
    "px.line(\n",
    "    raw_data,\n",
    "    x=raw_data.index,\n",
    "    y=raw_data[\"avg_daily_attendances\"],\n",
    "    title=\"Calendar adjusted data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjustments should also be made for population variation. For example, if plotting the number of hospital beds over time, we should account for growth in the population. Even though beds may have increased, beds per 1000 population may have decreased (this depends on the question you are asking from the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series data can be seperated into three seperate components:\n",
    "\n",
    "* **Trend** - Long term increase/decrease in data (not necessarily linear).\n",
    "* **Seasonal** - The impact of the data from seasonal factors, such as month of year or day of week.\n",
    "* **Remainder** - Everything else (i.e., random variation that can't be modelled or predicted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, each point in our data can be described with the following equation (additive model):<br>\n",
    "$y_t = T_t + S_t + R_t$, where:\n",
    "\n",
    "* $y_t$ is a data point in our dataset,\n",
    "* $T_t$ is the trend component, \n",
    "* $S_t$ the seasonality component,\n",
    "* and $R_t$ the remainder component, at time $t$.\n",
    "\n",
    "Note, the trend component includes both trend and cyclic behaviour. The cyclic component represents non-periodic fluctuation in the data (i.e., the impact from flu-season)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, we could have multiplicative decomposition, where we multiply each component:<br>\n",
    "$y_t = T_t \\times S_t \\times R_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the magnitude of seasonal fluctuations is independent of the trend, use an additive model, otherwise, use a multiplicative model.** Most healthcare applications will assume an additive model, usually multiplicative models are found in finance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset for multiplicative seasonality\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv\"\n",
    ")\n",
    "airline_df = pd.read_csv(url)\n",
    "airline_df[\"Month\"] = pd.to_datetime(airline_df[\"Month\"], format=\"%Y-%m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot example of multiplicative time series data\n",
    "px.line(\n",
    "    airline_df,\n",
    "    x=\"Month\",\n",
    "    y=\"Passengers\",\n",
    "    title=\"Example airline dataset to show multiplicative model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rolling averages are a sequence of averages taken from a sliding window within you data. For example, if we were to calculate a rolling average of window size 12, we would take the first 12 measurements, calculate the mean for the first value. We then slide the window along one unit and calculate the mean for the next 12 measurements, and so on.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be plotted\n",
    "rolling_fig = px.scatter(\n",
    "    raw_data,\n",
    "    y=\"avg_daily_attendances\",\n",
    "    trendline=\"rolling\",\n",
    "    trendline_options=dict(window=12),\n",
    "    title=\"12-month moving average\",\n",
    ")\n",
    "rolling_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to calculate this in pandas? By using the `.rolling()` method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[\"avg_daily_attendances\"].rolling(window=12).mean().head(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also center the average, so that it evenly represents the average over the 12 months (i.e., the above method calculates the average for the previous 12 months). This is done by usng the `center = True` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[\"avg_daily_attendances\"].rolling(window=12, center=True).mean().head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be better for estimating the trend component, since there is less lag between when a change in trend occurs in the data, and when it is detected by the rolling mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use a 24- month centered rolling average to get an estimate of the trend component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_period = 12\n",
    "# Rolling mean to smooth out our data\n",
    "trend_component = (\n",
    "    raw_data[\"avg_daily_attendances\"].rolling(12 * 2 + 1, center=True).mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a dataframe to store our time series components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_data = pd.DataFrame()\n",
    "decomposed_data.index = raw_data.index\n",
    "decomposed_data[\"original\"] = raw_data[\"avg_daily_attendances\"]\n",
    "decomposed_data[\"trend_component\"] = trend_component.values\n",
    "\n",
    "# Remove missing values\n",
    "decomposed_data = decomposed_data.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First (assuming an additive series) we remove trend from our data. Note, if it was a multiplicative series, we would simply divide by the trend component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract trend from our data\n",
    "seasonal_and_remainder = raw_data[\"avg_daily_attendances\"] - trend_component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then group by each month and take the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_arr = seasonal_and_remainder.index.month\n",
    "month_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_component = seasonal_and_remainder.groupby(by=month_arr).mean()\n",
    "seasonal_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLotting seasonal variation in attendance data\n",
    "seasonal_fig = px.bar(seasonal_component, title = 'Seasonal component in attendance data')\n",
    "seasonal_fig.update_layout(yaxis_title = 'seasonal variation', \n",
    "                           xaxis_title = 'Month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add the seasonal component to `decomposed_data` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_data[\"seasonal_component\"] = decomposed_data.index.month.map(\n",
    "    seasonal_component\n",
    ")\n",
    "decomposed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remainder component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder is simply the difference between the original series and the trend and seasonal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_data[\"remainder_component\"] = (\n",
    "    decomposed_data[\"original\"]\n",
    "    - decomposed_data[\"trend_component\"]\n",
    "    - decomposed_data[\"seasonal_component\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the remainder terms should ideally have no correlation and have mean 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=4, cols=1)\n",
    "\n",
    "fig.append_trace(\n",
    "    go.Scatter(\n",
    "        x=decomposed_data.index, y=decomposed_data[\"trend_component\"], name=\"Trend\"\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "\n",
    "fig.append_trace(\n",
    "    go.Scatter(\n",
    "        x=decomposed_data.index,\n",
    "        y=decomposed_data[\"seasonal_component\"],\n",
    "        name=\"Seasonal\",\n",
    "    ),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.append_trace(\n",
    "    go.Scatter(\n",
    "        x=decomposed_data.index,\n",
    "        y=decomposed_data[\"remainder_component\"],\n",
    "        name=\"Remainder\",\n",
    "    ),\n",
    "    row=3,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.append_trace(\n",
    "    go.Scatter(x=decomposed_data.index, y=decomposed_data[\"original\"], name=\"Original\"),\n",
    "    row=4,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "\n",
    "fig.update_layout(height=900, width=950, title_text=\"Decomposed time series\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit long-winded.. However, we can use `statsmodels` library to decompose the time series for us in only a few lines of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### statsmodels seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All our steps above are now condensed into one cell with 3 lines of code!\n",
    "decomposed_ts_update = seasonal_decompose(\n",
    "    raw_data[\"avg_daily_attendances\"].values, model=\"additive\", period=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=3, cols=1)\n",
    "\n",
    "fig.append_trace(\n",
    "    go.Scatter(\n",
    "        x=raw_data.index, y=decomposed_ts_update.trend, name=\"statsmodels trend\"\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.append_trace(\n",
    "    go.Scatter(\n",
    "        x=decomposed_data.index,\n",
    "        y=decomposed_data[\"trend_component\"],\n",
    "        name=\"manual trend\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "\n",
    "fig.append_trace(\n",
    "    go.Scatter(\n",
    "        x=raw_data.index, y=decomposed_ts_update.seasonal, name=\"statsmodels seasonal\"\n",
    "    ),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "fig.append_trace(\n",
    "    go.Scatter(\n",
    "        x=decomposed_data.index,\n",
    "        y=decomposed_data[\"seasonal_component\"],\n",
    "        name=\"manual seasonal\",\n",
    "    ),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "\n",
    "fig.append_trace(\n",
    "    go.Scatter(\n",
    "        x=raw_data.index, y=decomposed_ts_update.resid, name=\"statsmodels remainder\"\n",
    "    ),\n",
    "    row=3,\n",
    "    col=1,\n",
    ")\n",
    "fig.append_trace(\n",
    "    go.Scatter(\n",
    "        x=decomposed_data.index,\n",
    "        y=decomposed_data[\"remainder_component\"],\n",
    "        name=\"manual remainder\",\n",
    "    ),\n",
    "    row=3,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    width=950,\n",
    "    title_text=\"Comparing statsmodels seasonal_decompose to our manual calculations\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar outputs! Although the statsmodels approach requires much less work.. There are other methods to calculate the decomposed series, other than this 'classical approach', such as LOESS (available in statsmodels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Time series decomposition key points</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Time series decomposition allows you to understand underlying patterns and structures within the data. For example, what is the seasonal variation? Which months see the highest demand? What is the underlying trend for demand?\n",
    "* Although not typically used for forecasting, it helps identify which components may be more predictable and which model may be appropriate (more on this in the next session).\n",
    "* Decomposition can compliment other time series techniques, such as SPC analysis or causal impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (15-20 mins)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hospital manager has collected daily data on the number of admissions to the A&E department. They want to use a more data driven approach to calculate the weekly staffing rotas and quantify how demand has changed during the data collection period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating some fake time series data\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "time_index = pd.date_range(start=\"2022-01-01\", end=\"2024-05-01\", freq=\"D\")\n",
    "\n",
    "seasonal_component = (10 * np.sin((2 * np.pi / 7) * np.arange(len(time_index)))).astype(\n",
    "    int\n",
    ")\n",
    "remainder_component = np.random.normal(loc=0, scale=1.5, size=len(time_index)).astype(\n",
    "    int\n",
    ")\n",
    "trend_component = np.round(np.arange(len(time_index)) / 35).astype(int)\n",
    "\n",
    "time_series = 50 + seasonal_component + trend_component + remainder_component\n",
    "\n",
    "# Format data in a pandas dataframe\n",
    "task_data = pd.DataFrame({\"date\": time_index, \"admissions_daily\": time_series})\n",
    "task_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make a time series plot of daily A&E admissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using statsmodels `seasonal_decompose`, decompose the data into its individual components (use an additive model and think about the seasonal period for daily data). Plot each seasonal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. As we've made fake data, we can actually extract exactly the underlying trend and seasonal component (saved as `actual_seasonal_component` & `actual_trend_component`, in the cell below). How good was `seasonal_decompose` at isolating trend and components from the added Guassian noise? Plot the actual trend and seasonal component against the modelled components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_seasonal_component = (\n",
    "    10 * np.sin((2 * np.pi / 7) * np.arange(len(time_index)))\n",
    ").astype(int)\n",
    "actual_trend_component = np.round(np.arange(len(time_index)) / 35).astype(int) + 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** You can use the `.trend` and `.seasonal` to extract trend and seasonal data from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the regression notebook, we discussed the correlation coefficient (a measure of linear association between two variables).\n",
    "\n",
    "**The autocorrelation function (ACF) is defined as strength of the linear relationship between a time series and a lagged version of the same time series.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at our A&E NHS attendances dataset to understand this better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***First lag:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[\"avg_daily_attendances_lag1\"] = raw_data[\"avg_daily_attendances\"].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect linear relationship using a scatter plot\n",
    "px.scatter(\n",
    "    raw_data,\n",
    "    x=\"avg_daily_attendances_lag1\",\n",
    "    y=\"avg_daily_attendances\",\n",
    "    title=\"Relationship between avg_daily_attendances and the first lag\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[[\"avg_daily_attendances\", \"avg_daily_attendances_lag1\"]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Second lag:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[\"avg_daily_attendances_lag2\"] = raw_data[\"avg_daily_attendances\"].shift(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[[\"avg_daily_attendances\", \"avg_daily_attendances_lag2\"]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than manually calculate each lag and then the coefficient, we typically use an ACF plot which summarises this information. We have focused primarily on Plotly, but for here, we'll use matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "_ = statsmodels.graphics.tsaplots.plot_acf(\n",
    "    raw_data[\"avg_daily_attendances\"],\n",
    "    ax=ax,\n",
    "    lags=12 * 2,\n",
    "    title=\"Autocorrelation for A&E average daily attendances\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue shaded region is used to infer whether the correlation is significantly different to zero (at the 5% significance level)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>ACF key points</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Trending time series tends to have large positive correlations for small lags, since nearby observations have a similar value in a trending time series.\n",
    "* Seasonal data will have correlations that are larger at the seasonal lags (i.e., 12 lags for monthly data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 (15-20 mins)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the `task_data` calculate the first 2 lags as columns in the pandas dataframe (i.e., create new columns in the dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the `pandas.corr()` method to get the correlation coefficient for these two lags to the target column (`admissions_daily`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Plot the autocorrelation function, using statsmodels, for the previous 4 weeks of lags. By reading the plot, do you agree with the decomposed graphs derived in the first task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
