{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518f7e1e-c7c0-4186-9d24-8d4ae052f006",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Data preprocessing is an important step in machine learning and is a prerequisite for modelling your data. <br>\n",
    "\n",
    "The main goal of data preprocessing is to transform your dataset into a suitable form for modelling, and in doing this it will also improve the performance of your model and provide more robust and reliable results.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c00a1c-db12-4a16-9e38-5f7f3a6b9f2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleaning and Preparing Data\n",
    "\n",
    "There are many things to consider for preprocessing and not all of them will be relevant to your data, but still should be checked for. Gathering, cleaning, and preparing your data will take, as is expected to take, a significant proportion of your time and attention when compared with the time spent on model development and model tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8d9d25-9934-445a-a8f3-cd294ad934da",
   "metadata": {
    "tags": []
   },
   "source": [
    "Within this chapter the following topics will be covered:<ul>\n",
    "- Checking and Converting Data Types\n",
    "\n",
    "- Handle Missing Values\n",
    "    - Remove Records with Missing Values\n",
    "    - Removing Specific Columns Or Rows That Contain Missing Data\n",
    "    - Impute Missing Values\n",
    "    \n",
    "- Remove Duplicates\n",
    "\n",
    "- Removing Outliers\n",
    "    - Interquartile Range (IQR) method\n",
    "    - Z-score (Standard Score)\n",
    "    \n",
    "- Dealing with Target Imbalance\n",
    "    - SMOTE\n",
    "    - Random Overs-sampling\n",
    "    - Random Under-sampling         </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4216927-65fa-4283-9642-35f957332555",
   "metadata": {},
   "source": [
    "Import the following libraries for this chapter:\n",
    "\n",
    "> It is good practice to import all the libraries that are used in your notebook at the very start. You may not know all the libraries you intend on using from the beginning, but they should all be added here as and when you need additional libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ebccb8-14d3-429d-8104-142645be8d86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T12:20:08.939066400Z",
     "start_time": "2024-03-22T12:20:05.221544400Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Removing Outliers section\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dealing with Target Imbalance section\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981f518c-7bc3-45fd-8d1f-40e00d2cc746",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Checking and Converting Data Types\n",
    "\n",
    "Before any preprocessing can begin, we must understand the data types of our features. <br>\n",
    "`Pandas` will automatically assign data types for your data when you import a dataset. Sometimes the types assigned will be incorrect, so it is important to check these and convert them to the correct data type.<br>\n",
    "The most common data types used in `pandas` are:\n",
    " - `object`: Contains sting values or contains a mixture of types.\n",
    " - `int64`: Whole numbers, equivalent to native python integer type, where 64 refers to the allocation of memory allotted for storing the value, in this case, the number of bits.\n",
    " - `float64`: Decimal numbers.\n",
    " - `datetime64`: Dates and times - this special data type unlocks a extra functionality for working with time series data, such as datetime indexing.\n",
    " \n",
    " We have also seen in the previous module data type `category` specifically used for categorical data when using `Pandas`.\n",
    " \n",
    " Data types can be easily checked in a dataset by using the `.info()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc938d9-0242-4be7-95c7-0cde38465d95",
   "metadata": {
    "tags": []
   },
   "source": [
    "Generate the below dataset from the function `generate_demo_dataset1` and letâ€™s look at the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9063cc-7656-4774-91f4-040e6ba23dbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T12:20:08.948070500Z",
     "start_time": "2024-03-22T12:20:06.534629700Z"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# generate_demo_dataset1\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "def generate_demo_dataset1():\n",
    "    \"\"\"\n",
    "    Generate a dataset for cleaning data examples.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the generated dataset with columns:\n",
    "    'category', 'a', 'b', 'c', 'd'.\n",
    "    \"\"\"\n",
    "    category = ['category 1', 'category 2', 'category 2', 'category 1', 'category 1', 'category 1']\n",
    "    a_values = (6.0, 2.0, 4.0, 3.0, 7.0, 5.0)\n",
    "    b_values = [6.0, 3.0, 5.0, 1.0, 10.0, 8.0]\n",
    "    c_values = [9.0, 4.0, 3.0, 3.0, 7.0, 1.0]\n",
    "    d_values = [1.0, 2.0, 7.0, 9.0, 6.0, 2.0]\n",
    "\n",
    "    # Create DataFrame\n",
    "    data = {'category': category, 'a': a_values, 'b': b_values, 'c': c_values, 'd': d_values}\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    df['c'] = df['c'].astype(\"str\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df = generate_demo_dataset1()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae60a38a-ec0b-4564-b660-d25de0cf9650",
   "metadata": {
    "tags": []
   },
   "source": [
    "To inspect the data types use `df.info()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14767da3-2900-4578-a5ad-505c203531bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T12:20:08.949071400Z",
     "start_time": "2024-03-22T12:20:06.541013800Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = generate_demo_dataset1() # original dataframe\n",
    "# print(df,\"\\n\\n\") # print dataframe with two carriage returns.\n",
    "\n",
    "# inspect the data types of the dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084f4e20-4c0b-438d-b0d3-0c5ff7788fd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "The pandas `.astype()` method can be used to convert a column's data type to a specified data type. <br>\n",
    "In order to do this we need to reassign the column to overwrite the original data when converting it.<br>\n",
    "\n",
    "Before converting a column be extra careful that all the values it contains can be appropriately converted to the new data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba83ea0-5bb6-405c-b4ce-bf45ad76a02e",
   "metadata": {},
   "source": [
    "Let's check the values in column `\"c\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc7f1cd-18d3-45c7-aa01-751560130b62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T12:20:08.949071400Z",
     "start_time": "2024-03-22T12:20:06.560418300Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = generate_demo_dataset1() # original dataframe\n",
    "# print(df,\"\\n\\n\") # print dataframe with two carriage returns.\n",
    "\n",
    "# Check the age column for unique values and it's data type\n",
    "unique_values = df[\"c\"].unique()\n",
    "\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7dd52e-f90c-4795-984d-0a632d400974",
   "metadata": {},
   "source": [
    "Check that column `\"c\"` is in fact data type `'object'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99083c71-7d44-47c0-a03b-2ce23ae3ad6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T12:20:08.949071400Z",
     "start_time": "2024-03-22T12:20:06.571553200Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = generate_demo_dataset1() # original dataframe\n",
    "# print(df,\"\\n\\n\") # print dataframe with two carriage returns.\n",
    "\n",
    "# Check the data type of the column\n",
    "column_dtype = df['c'].dtype\n",
    "\n",
    "print(column_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f254200-9483-4bec-a7e0-d262dee1bdf2",
   "metadata": {},
   "source": [
    "To change column `\"c\"` to data type `'float'` we need to overwrite the original column with the new data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a222c-04cd-40af-8682-429ad6a4cd33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T12:20:08.949071400Z",
     "start_time": "2024-03-22T12:20:06.581021Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = generate_demo_dataset1() # original dataframe\n",
    "# print(df,\"\\n\\n\") # print dataframe with two carriage returns.\n",
    "\n",
    "# Change the age column data type to integer\n",
    "df[\"c\"] = df[\"c\"].astype(\"float\")\n",
    "\n",
    "# Check the data type of the column\n",
    "column_dtype = df['c'].dtype\n",
    "\n",
    "print(column_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acd924a-dffe-4941-b094-59b5433ee159",
   "metadata": {
    "tags": []
   },
   "source": [
    "<b>Practical Task 1.1</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2ae502-0dd2-4aa1-a06f-0be00acb7fee",
   "metadata": {},
   "source": [
    "Inspect the following dataset. Identify and convert two columns that require their data type to be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e848a442-f115-47e6-9153-698e0a39766c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T12:20:08.950070900Z",
     "start_time": "2024-03-22T12:20:06.595566200Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate_practical_dataset1\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "def generate_practical_dataset1():\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset consisting of patient demographic information\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the synthetic dataset with columns: \n",
    "    'Surname', 'Age', 'Gender', 'Weight', 'Blood Type'.\n",
    "    \"\"\"\n",
    "    # List of patient surnames\n",
    "    patient_names = [\"Smith\", \"Johnson\", \"Williams\", \"Jones\", \"Brown\",\n",
    "                     \"Davis\", \"Miller\", \"Wilson\", \"Moore\", \"Taylor\"]\n",
    "\n",
    "    # List of blood types\n",
    "    blood_types = [\"A+\", \"A-\", \"B+\", \"B-\", \"AB+\", \"AB-\", \"O+\", \"O-\"]\n",
    "\n",
    "\n",
    "    # Generate random demographic dataset\n",
    "    demographic_dataset = []\n",
    "\n",
    "    for n in patient_names:\n",
    "        name = n\n",
    "        age = random.randint(20, 85)\n",
    "        gender = random.choice([\"Male\", \"Female\"])\n",
    "        weight = round(random.uniform(55.5,95.5),2)\n",
    "        blood_type = random.choice(blood_types)\n",
    "        record = {\n",
    "            \"Surname\": name,\n",
    "            \"Age\": age,\n",
    "            \"Gender\": gender,\n",
    "            \"Weight\": weight,\n",
    "            \"Blood Type\": blood_type,\n",
    "        }\n",
    "        demographic_dataset.append(record)\n",
    "\n",
    "    # Convert dataset to DataFrame\n",
    "    df = pd.DataFrame(demographic_dataset)\n",
    "    \n",
    "    # Set all columns to 'object' data types\n",
    "    df = df.astype('object')\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "df = generate_practical_dataset1()\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd7f857-2d4e-44dc-9ab3-746cad400f8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T12:20:08.950070900Z",
     "start_time": "2024-03-22T12:20:06.607644900Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your Code Here - Converting Data Types (Identify and convert two columns)\n",
    "# df = generate_practical_dataset1()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b0ce3-a3e2-4ce4-8006-1b5fa5df51fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Handle Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1806c-c3e0-45f2-ae43-c6822330eb40",
   "metadata": {},
   "source": [
    "Datasets often have missing values or empty records, often encoded as blanks or NaN (Not a Number). Handling missing values is the most common problem in data science and is the first step of data preprocessing as most machine learning algorithms can't deal with values that are missing or blank. \n",
    "\n",
    "Removing ALL records with missing values is a basic strategy that is sometimes used, but it comes with a cost of losing probable valuable data and the associated information or patterns. A better strategy is to impute the missing values. \n",
    "\n",
    "In this next section we are going to take a look at:: <br>\n",
    " - Remove all records with missing values.\n",
    " - Removing specific columns or rows.<br>\n",
    " - Impute (fill-in) the missing values.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293cdfa5-66dd-4d09-9f80-2163064092c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "Generate the below dataset from the function `generate_demo_dataset2` and let's look at the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6bee7c-171e-4e7f-b7ce-216f97fbb410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T12:20:08.950070900Z",
     "start_time": "2024-03-22T12:20:06.618276900Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate_demo_dataset2\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "def generate_demo_dataset2():\n",
    "    \"\"\"\n",
    "    Generate a dataset for cleaning data.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the generated dataset with columns: \n",
    "    'category', 'a', 'b', 'c', 'd'.\n",
    "    \"\"\"\n",
    "\n",
    "    category = ['category 1','category 2','category 2',None,'category 1','category 1']\n",
    "    a_values = (6.0,np.nan,4.0,3.0,7.0,5.0)\n",
    "    b_values = [np.nan,3.0,5.0,np.nan,10.0,8.0]\n",
    "    c_values = [9.0,4.0,3.0,3.0,7.0,1.0]\n",
    "    d_values = [1.0,2.0,7.0,np.nan,6.0,2.0]\n",
    "\n",
    "\n",
    "    # Create DataFrame\n",
    "    data = {'category': category, 'a': a_values, 'b': b_values, 'c': c_values, 'd': d_values}\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "df = generate_demo_dataset2()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83058635-6eb6-49d3-af96-59abf0c64437",
   "metadata": {},
   "source": [
    "#### Remove all records with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c3e926-5c93-48ec-9b99-14ca2f5de2d7",
   "metadata": {},
   "source": [
    "In Python, particularly in `pandas` dataframes or `numpy` arrays, `NaN` is commonly used to represent missing or undefined numerical data. However, if you're working with non-numeric data types, such as objects, `None` is often used as an alternative to represent missing values.<br>\n",
    "\n",
    "Within SQL `NULL` is more broadly used to represent missing values across different data types.\n",
    "\n",
    "We can use the `.isna()` method to inspect the `NaN` values. <br>\n",
    "This will return a boolean result (True or False) on whether the value <br>\n",
    " - is `NaN` - True <br> \n",
    " or <br>\n",
    " - Not `Nan` - False <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a28a63a-05d9-4272-91c7-bc8827dadd60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T12:20:09.014068600Z",
     "start_time": "2024-03-22T12:20:08.918338700Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = generate_demo_dataset2() # original dataframe\n",
    "print(df,\"\\n\\n\") # print dataframe with two carriage returns.\n",
    "\n",
    "# Identify missing values - ie: Is the value missing?\n",
    "df.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd9317-d532-4a12-afe3-252f848deefd",
   "metadata": {},
   "source": [
    "In pandas, `isna()` and `isnull()` are essentially aliases of each other, meaning they are two different names for the same function. Both functions are used to detect missing values in a dataframe or series. There is no difference in functionality between them; you can use either one based on your preference.\n",
    "\n",
    "Similarly, `notna()` and `notnull()` are also aliases of each other and serve the same purposeâ€”to detect non-missing values in a dataframe or series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88ab661-710d-48e7-8556-e9443b7b6064",
   "metadata": {},
   "source": [
    "So looking at our dataset we may decide to remove (or `drop`) <b>all</b> the rows where `NaN`/`None` values are present in any of the columns. <br>\n",
    "For this we can use: `.dropna()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9384117-9ae9-4294-abbb-b42e4dc392fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = generate_demo_dataset2() # original dataframe\n",
    "print(df,\"\\n\\n\") # print dataframe with two carriage returns.\n",
    "\n",
    "# drop all rows in the dataframe that contain NaN/None values in any of the columns\n",
    "drop_all_nan_rows = df.dropna()\n",
    "\n",
    "print(drop_all_nan_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07061dee-c9cc-4b42-887c-2f605f8da338",
   "metadata": {},
   "source": [
    "This is a very and broad approach to dealing with missing values. There are more specific ways that we might choose to adopt instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b01bae-e6ef-4739-bb10-665d2945b409",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Removing specific columns or rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aaeaa0-3b6d-4aac-8c79-0f76d07cb0ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can drop specific rows by passing index labels to the `.drop()` function.<br>\n",
    "\n",
    "The `.drop()` function does not check for `NaN` or `None` Values. <br>\n",
    "By passing the index labels to this function the rows will be just be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3889f0c4-a900-4316-aed5-6c466a5ab705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = generate_demo_dataset2() # original dataframe\n",
    "print(df,\"\\n\\n\") # print dataframe with two carriage returns.\n",
    "\n",
    "# Drop rows in the dataframe by their specified row index\n",
    "drop_specified_rows = df.drop([1,2,4])\n",
    "\n",
    "print(drop_specified_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1eb138-fa16-4270-8a58-b865ccecc334",
   "metadata": {
    "tags": []
   },
   "source": [
    "Alternatively, there may be certain columns that are not required. <br>\n",
    "These can also be removed using the `.drop()` function by also passing the `axis` value = 1, which indicates a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0e1475-ae15-4df2-acf3-b5d32fba45db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = generate_demo_dataset2() # original dataframe\n",
    "print(df,\"\\n\\n\") # print dataframe with two carriage returns.\n",
    "\n",
    "# Drop a specified column from the dataframe\n",
    "drop_column = df.drop(\"b\",axis=1)\n",
    "\n",
    "print(drop_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88ab292-a23e-445b-9d11-d48e7fd1c8bb",
   "metadata": {},
   "source": [
    "What if we want to drop rows only where data is missing in a particular column? <br>\n",
    "So far we have just looked at dropping rows and columns without checking for missing values using `.drop()`. \n",
    "Let's look at what we can do specifically considering missing values. <br>\n",
    "We can use `.dropna()` in the same way as `.drop()`, but it will cater to only rows or columns with `NaN` or `None` values.\n",
    "\n",
    "For this, first take a look at how many missing values we have in each column using `.isna()` method to identify the number of `NaN` values and then additionally using the `.sum()` method to count them in each column. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb3ee7-35f6-43d5-a307-8718ecd03d00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = generate_demo_dataset2() # original dataframe\n",
    "print(df,\"\\n\\n\") # print dataframe with two carriage returns.\n",
    "\n",
    "# Count the number of NaN values in each column\n",
    "CountNaN = df.isna().sum()\n",
    "\n",
    "print(CountNaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a852f6-865e-4bb1-9f54-576252b900b8",
   "metadata": {},
   "source": [
    "To remove the rows with missing values in a particular column we can specify a list of labels to the `subset` argument of `.dropna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb7ab7-a5b1-459c-a57d-0352f3f7e1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = generate_demo_dataset2() # original dataframe\n",
    "print(df,\"\\n\\n\") # print dataframe with two carriage returns.\n",
    "\n",
    "# Drop rows from a dataframe where NaN values are present in a specified column\n",
    "SpecifiedColumn = df.dropna(subset=[\"b\"])\n",
    "\n",
    "print(SpecifiedColumn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dde91e-94e0-4fd9-bb4d-ff0bcd955787",
   "metadata": {},
   "source": [
    "For more options on dropping values where `NaN` values exits, see the pandas documentation on dropna [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a72a838-27cf-49bf-85c9-2bc460674dd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Impute the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c555d3-2f79-44b9-a7df-1549aa45665a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Rather than removing rows or columns that have missing data we could fill in the missing values using the measures of central tendency, such as mean, median, and mode.<br>\n",
    " - The mean can be used to impute a numeric feature.<br>\n",
    " - The median can be used to impute the ordinal feature. <br>\n",
    " - The mode or highest occurring value can be used to impute the categorical feature. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84d478b-738e-4f1d-a041-2f2c6a7ea9ea",
   "metadata": {},
   "source": [
    "> Note: it is important to understand that in some cases, missing values will not impact the data, such as unique identifiers.<br> For example, unique values such as MRN, NHS Number will not impact the machine learning models because they are just identifiers shouldn't be used as features in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac250836-6793-4021-a768-5d539b592a27",
   "metadata": {},
   "source": [
    "Letâ€™s first use the `.describe()` method to review statistics on the numerical columns within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bac6d7e-982a-407b-a7ad-8a737ab8b828",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = generate_demo_dataset2() # original dataframe\n",
    "print(df,\"\\n\\n\") # print dataframe with two carriage returns.\n",
    "\n",
    "# stats on numerical values\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd86b88-a844-49a1-90a9-6da43000242b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Letâ€™s use the `mean` to fill in the missing values for column `\"a\"` and the `median` for column `\"b\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a1224-5a06-4f8b-b0c3-78e2128df7de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = generate_demo_dataset2() # original dataframe\n",
    "print(df,\"\\n\\n\") # print dataframe with two carriage returns.\n",
    "\n",
    "# stats on numerical values\n",
    "print(df.describe(),\"\\n\\n\")\n",
    "\n",
    "# fill NaN values in column a with mean and median\n",
    "df[\"a\"] = df[\"a\"].fillna(df[\"a\"].mean())\n",
    "df[\"b\"] = df[\"b\"].fillna(df[\"b\"].median())\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18007b6f-ff44-49f1-b53c-772eb4d1b78a",
   "metadata": {
    "tags": []
   },
   "source": [
    "And letâ€™s now use the most frequently occurring value in `category` to replace the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c847da-4641-4adf-9f63-278aeec048b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = generate_demo_dataset2() # original dataframe\n",
    "print(df,\"\\n\\n\") # print dataframe with two carriage returns.\n",
    "\n",
    "# fill NaN values in column category (categorical feature) a with mode\n",
    "df[\"category\"] = df[\"category\"].fillna(df[\"category\"].mode()[0])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dba0b5-f7b4-42a9-8ac7-c28a7ccd1a22",
   "metadata": {},
   "source": [
    "We may decide that it is better to replace missing values with a specified value instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0529fde8-a8a2-4e58-8590-50f2ae438be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = generate_demo_dataset2() # original dataframe\n",
    "print(df,\"\\n\\n\") # print dataframe with two carriage returns.\n",
    "\n",
    "# fill NaN values in column category (categorical feature) a with specified value\n",
    "df[\"category\"] = df[\"category\"].fillna(\"Unknown\")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19501693-eac9-4ba9-b754-fdbe907097a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<b>Practical Task 1.2</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a6d05-8ffe-4968-b650-30b06fb478df",
   "metadata": {
    "tags": []
   },
   "source": [
    "From the following dataset look to carry out the following: <br>\n",
    " - Drop at least one column that you feel that wouldn't be needed in your model.\n",
    " - Drop rows where there are nulls in a specific column.\n",
    " - Fill in missing values using either mean, median or mode within a column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a0a1a6",
   "metadata": {},
   "source": [
    "> Note: If you want to ensure the changes are saved to the original DataFrame without creating a new one, setting `inplace=True` needs to be added as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24303bea-d34b-4731-9fb4-bd2b413c0ece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate_practical_dataset2\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "def generate_practical_dataset2():\n",
    "    \"\"\"\n",
    "    Generate a synthetic demographic dataset.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the generated dataset with columns: \n",
    "    'MRN', 'Surname', 'Age', 'Gender', 'Favourite Colour', 'Weight', 'Blood Type'.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "    'MRN':    [482754, 194552, 456272, 569149, 152106, 697630, 922086, 801114, 942324, 737040],\n",
    "    'Surname': ['Smith', 'Johnson', 'Williams', 'Jones', 'Brown', 'Davis', 'Miller', 'Wilson', 'Moore', 'Taylor'],\n",
    "    'Age': [26, np.NaN, 56, 54, 74, 62, 83, 24, np.NaN, 26],\n",
    "    'Gender': ['Male', 'Female', 'Male', None, 'Male', 'Female', None, 'Female', None, 'Female'],\n",
    "    'Favourite Colour': ['Red',  np.NaN, 'Purple', 'Blue', 'Orange', 'Red',  np.NaN, 'Yellow', 'Black', 'Pink'],\n",
    "    'Weight': [57.87, 66.96, 62.98, 63.83, 87.95, 69.91, np.NaN, 61.49, 93.71, np.NaN],\n",
    "    'Blood Type': ['A+', 'B-', 'B-', 'AB+', 'A-', 'O+', 'AB+', 'B+', 'B-', 'O+']\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    return (df)\n",
    "\n",
    "df = generate_practical_dataset2()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1114c3-5e8a-4eb1-9eac-ec6e334ea11d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df = generate_practical_dataset2()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b719a9",
   "metadata": {},
   "source": [
    "There are other methods of imputing missing values such as [sklearn.impute.IterativeImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#:~:text=Multivariate%20imputer%20that%20estimates%20each,more%20in%20the%20User%20Guide) and [sklearn.impute.KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html#:~:text=Imputation%20for%20completing%20missing%20values,neither%20is%20missing%20are%20close). Use the links to learn more about these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79046a2d-20e3-4da5-aaf6-8b531761de74",
   "metadata": {},
   "source": [
    "### Remove Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb84f726-f699-45f8-a060-9d2630a3af27",
   "metadata": {},
   "source": [
    "Here we are going to look at how to identify duplicate records in your dataset and how to remove these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea10d59c-2939-459d-88c3-a7b9872f292f",
   "metadata": {},
   "source": [
    "Generate the below dataset from the function `generate_demo_dataset3` and letâ€™s take a look at the duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf270631-0257-46e8-af5d-06fbcd1350f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate_demo_dataset3\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "def generate_demo_dataset3():\n",
    "    \"\"\"\n",
    "    Generate a dataset for cleaning data.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the generated dataset with columns: \n",
    "    'category', 'a', 'b', 'c', 'd', 'colour'.\n",
    "    \"\"\"\n",
    "\n",
    "    category = ['category 1','category 2','category 2','category 1','category 1','category 1','category 1','category 1','category 1']\n",
    "    a_values = (6.0,1.0,1.0,3.0,7.0,5.0,3.0,7.0,5.0)\n",
    "    b_values = [7.0,3.0,5.0,3.0,10.0,8.0,3.0,10.0,8.0]\n",
    "    c_values = [9.0,4.0,3.0,3.0,7.0,1.0,3.0,7.0,1.0]\n",
    "    d_values = [1.0,2.0,7.0,3.0,6.0,2.0,3.0,6.0,2.0]\n",
    "    colour = ['Red','Blue','Green','Orange','Yellow','Pink','Orange','Yellow','Pink']\n",
    "\n",
    "\n",
    "    # Create DataFrame\n",
    "    data = {'category': category, 'a': a_values, 'b': b_values, 'c': c_values, 'd': d_values, 'colour': colour}\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "df = generate_demo_dataset3()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62eff9b-e239-4986-b694-cc27f7cb446c",
   "metadata": {},
   "source": [
    "`.duplicated()` can be used to identify duplicate records and will return a Boolean value for the record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26915143-103a-44ed-937c-0d8d1ecc457b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = generate_demo_dataset3() # original dataframe\n",
    "print(df,\"\\n\\n\") # print dataframe with two carriage returns.\n",
    "\n",
    "df.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c6208-c86b-44b2-bede-fc28c6388644",
   "metadata": {
    "tags": []
   },
   "source": [
    "If we want to view the duplicate records, we can carry out the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cca808-732c-44ed-a04b-1dbfb47c3d12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = generate_demo_dataset3() # original dataframe\n",
    "print(df,\"\\n\\n\") # print dataframe with two carriage returns.\n",
    "\n",
    "# Identify duplicate records\n",
    "duplicate_records = df.duplicated()\n",
    "\n",
    "# Select duplicate records\n",
    "duplicates = df[duplicate_records]\n",
    "\n",
    "# Display duplicate records\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7412d34c-56f5-42bb-b266-6533f04bfe09",
   "metadata": {},
   "source": [
    "To remove the duplicates completely from the dataset use the `.drop_duplicates()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a8db5b-2745-46e8-a4cc-122372363196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = generate_demo_dataset3() # original dataframe\n",
    "print(df,\"\\n\\n\") # print dataframe with two carriage returns.\n",
    "\n",
    "# Remove duplicates\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "\n",
    "print(df_no_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e883dfb-dc66-4cb1-aa78-f2931ade2e3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "We may wish to refine this further by passing the `keep` argument, where we can specify whether the first or last duplicate record should be kept. <br>\n",
    "If `keep` is not specified the default is the `first`. <br>\n",
    "An example of when you might want to keep the last record would be if you had a sorted dataframe in chronological order where you wanted to keep the most recent record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd89f7-e9b9-4a9e-9dfa-d0f620e60813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = generate_demo_dataset3() # original dataframe\n",
    "print(df,\"\\n\\n\") # print dataframe with two carriage returns.\n",
    "\n",
    "# Keep the last occurrence of each duplicated row\n",
    "df_no_duplicates = df.drop_duplicates(keep='last')\n",
    "\n",
    "print(df_no_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf8adb-2057-4f2a-80e7-d573b536df2d",
   "metadata": {},
   "source": [
    "By passing `False` to this argument <b>all</b> duplicates will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3297734-e442-4f53-90cf-5f98d99377c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = generate_demo_dataset3() # original dataframe\n",
    "print(df,\"\\n\\n\") # print dataframe with two carriage returns.\n",
    "\n",
    "# Drop all duplicates\n",
    "df_no_duplicates = df.drop_duplicates(keep=False)\n",
    "\n",
    "print(df_no_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf73c770-1829-4f0f-b179-92d3e71ba6e4",
   "metadata": {},
   "source": [
    "Instead of identifying duplicates across the whole set of columns, certain specified columns can be used to identify duplicates. <br>\n",
    "For this will use the `subset` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25be17f4-c9cb-43aa-9e6f-f29aeb591509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = generate_demo_dataset3() # original dataframe\n",
    "print(df,\"\\n\\n\") # print dataframe with two carriage returns.\n",
    "\n",
    "df_no_duplicates = df.drop_duplicates(subset=['category', 'a'])\n",
    "\n",
    "print(df_no_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e425e5-e0da-488a-843a-e685f48df858",
   "metadata": {
    "tags": []
   },
   "source": [
    "<b>Practical Task 1.3</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42769879-a11c-45d2-b3ce-bdeb88ae87dc",
   "metadata": {},
   "source": [
    "From the following dataset:\n",
    "- Identify the two duplicates in the data and remove them.\n",
    "- Inspect the results and remove any further suspected duplicates based on 'Surname' and 'Age'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d9567-e219-4d18-b052-b057b43c3cdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate_practical_dataset3\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "def generate_practical_dataset3():\n",
    "    \"\"\"\n",
    "    Generate a synthetic demographic dataset.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the generated dataset with columns: \n",
    "    'MRN', 'Surname', 'Age', 'Gender', 'Weight', 'Blood Type'.\n",
    "    \"\"\"\n",
    "\n",
    "    data = {\n",
    "        'MRN': [176968, 173798, 851542, 336291, 114317, 737813, 609203, 938757, 661284, 147859,336291,661284,319011],\n",
    "        'Surname': ['Smith', 'Johnson', 'Williams', 'Jones', 'Brown', 'Davis', 'Miller', 'Wilson', 'Moore', 'Taylor','Jones','Moore', 'Taylor'],\n",
    "        'Age': [26, 24, 56, 54, 74, 62, 83, 24, 60, 26,54,60, 26],\n",
    "        'Gender': ['Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Female', 'Female','Female','Female', None],\n",
    "        'Weight': [57.87, 66.96, 62.98, 63.83, 87.95, 69.91, 62.14, 61.49, 93.71, 94.47,63.83, 93.71, 94.47],\n",
    "        'Blood Type': ['A+', 'B-', 'B-', 'AB+', 'A-', 'O+', 'AB+', 'B+', 'B-', 'O+','AB+','B-', '0+']\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "df = generate_practical_dataset3()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314d9e45-47b2-45f2-bb8f-4001a65a99b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# df = generate_practical_dataset3()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf85f7f-ee5f-4217-8f99-0ca02c418746",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Removing Outliers\n",
    "\n",
    "This section we will briefly look into removing outliers, but this is just a smaller part of a much wider topic of anomaly detection that will be covered separately in it's own module. <br>\n",
    "Essentially anomaly detection encompasses two broad practices of 'outlier detection' and 'novelty detection'.\n",
    "\n",
    "Where outliers are abnormal or extreme data points that are only seen in your initial training and novelties are new or previously unseen instances compared to your original data.\n",
    "\n",
    "Getting back to looking at outliers, we are now going to take a look at some simple ways of identifying and removing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041fe7f-c4a8-4dc2-a931-fa1d62a8d45d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate_demo_dataset4\n",
    "# import pandas as pd\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_demo_dataset4():\n",
    "    \"\"\"\n",
    "    Generate a dataset with outliers.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: An array containing the generated dataset with possible outliers.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    data = np.random.normal(loc=8, scale=1, size=100) #loc=10\n",
    "    outlier_indices = np.random.choice(100, size=10, replace=False)  # Introduce 10 outliers\n",
    "    data[outlier_indices] = np.random.normal(loc=12, scale=1, size=10)  # Outliers have mean 12 #20,1,10\n",
    "   \n",
    "    return(data)\n",
    "\n",
    "data = generate_demo_dataset4()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b398744a-bbc0-4cf4-a11b-d1e210190891",
   "metadata": {
    "tags": []
   },
   "source": [
    "Popular methods of outlier detection that are used:\n",
    "\n",
    "- <b>Interquartile range (IQR)</b>: The IQR is the range between the first quartile (Q1) and the third quartile (Q3) of a distribution. <br>\n",
    "When an instance is beyond Q1 or Q3 for some multiplier of IQR, they are considered outliers. The most common multiplier is 1.5, making the outlier range `[Q1â€“1.5 * IQR, Q3 + 1.5 * IQR]`.\n",
    "\n",
    "- <b>Z-score (standard score) </b>: The z-score or standard score measures how many standard deviations a data point is away from the mean. <br> Generally, instances with a z-score over 3 are chosen as outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53672cc6-2e41-4282-92c4-de748e013dd2",
   "metadata": {},
   "source": [
    "Letâ€™s plot our data, which is in a numpy array, and take a look at the data points. <br>\n",
    "For this we will use a plotting library called `matplotlib` to generate a plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb18aa7",
   "metadata": {},
   "source": [
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871b2804-fb94-4fc6-bbfa-4ac507fe0f5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ab8c1-9d5a-40a3-9e02-7a9386c3ef07",
   "metadata": {},
   "source": [
    "Now letâ€™s plot the data as a box plot and what do you notice? <br>\n",
    "> To do this swap, `.plot` with `.boxplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a21583-475d-47c6-a557-e17e9ff3fa09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.boxplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccadf63-fd41-4bea-9d3e-65eadd7799ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "It is clear from the box plot that there are points that appear to be outliers in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b97e9b-a94c-4565-ace8-ee564921d86c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### IQR method\n",
    "We are first going to take a look at the IQR method. <br>\n",
    "The IQR is the range between the first quartile (Q1) and the third quartile (Q3) of a distribution. When an instance is beyond Q1 or Q3 for some multiplier of IQR, they are considered outliers. The most common multiplier is 1.5, making the outlier range [Q1â€“1.5 * IQR, Q3 + 1.5 * IQR]. <br>\n",
    "\n",
    "Generate the function and then use it to calculate the upper and lower bounds based on the IQR * 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f874c653-d88c-4a6c-a176-59fdc5e7652d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define function to identify outliers using IQR method\n",
    "def identify_outliers_iqr(data, threshold=1.5):\n",
    "    \"\"\"\n",
    "    Identify outliers in a dataset using the interquartile range (IQR) method.\n",
    "\n",
    "    Args:\n",
    "    data (numpy.ndarray or pandas.Series): The data for which outliers are to be identified.\n",
    "    threshold (float, optional): The threshold value to determine outliers. Defaults to 1.5.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - outliers (numpy.ndarray): A boolean array indicating outliers in the data.\n",
    "        - lower_bound (float): The lower bound for outlier detection.\n",
    "        - upper_bound (float): The upper bound for outlier detection.\n",
    "    \"\"\"\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - threshold * iqr\n",
    "    upper_bound = q3 + threshold * iqr\n",
    "    outliers = np.logical_or(data < lower_bound, data > upper_bound)\n",
    "    \n",
    "    return outliers, lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1028ca73-c099-44af-b453-aa51bd631431",
   "metadata": {
    "tags": []
   },
   "source": [
    "let's use the `identify_outliers_iqr` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6412d431-ecea-4b50-a28e-4c54efe911b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outliers_iqr, lower_bound, upper_bound = identify_outliers_iqr(data)\n",
    "\n",
    "print(outliers_iqr)\n",
    "print(\"lower_bound:\", lower_bound)\n",
    "print(\"upper_bound:\", upper_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b23840d-44d8-49fd-a4f0-5eacd51c9902",
   "metadata": {
    "tags": []
   },
   "source": [
    "and plot the results using the generated function below `plot_outliers_iqr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134d1014-ab6b-4b0b-bea0-66225c95312f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_outliers_iqr(data, outliers_iqr, lower_bound, upper_bound):\n",
    "    \"\"\"\n",
    "    Plot the data with identified outliers using the interquartile range (IQR) method.\n",
    "\n",
    "    Args:\n",
    "    data (numpy.ndarray or pandas.Series): The original data to be plotted.\n",
    "    outliers_iqr (numpy.ndarray): A boolean array indicating outliers in the data.\n",
    "    lower_bound (float): The lower bound for outlier detection.\n",
    "    upper_bound (float): The upper bound for outlier detection.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))  # create a blank figure to plot on\n",
    "\n",
    "    plt.plot(data, label='Data')  # plot the data\n",
    "    plt.plot(np.where(outliers_iqr)[0], data[outliers_iqr], 'ro', label='Outliers (IQR)')  # highlight the outliers\n",
    "\n",
    "    plt.axhline(lower_bound, color='gray', linestyle='--', label='Lower Bound')  # add lower bound line\n",
    "    plt.axhline(upper_bound, color='gray', linestyle='--', label='Upper Bound')  # add upper bound line\n",
    "\n",
    "    plt.legend()  # add legend\n",
    "    plt.xlabel('Day')  # add x label\n",
    "    plt.ylabel('Index')  # add y label\n",
    "    plt.title('Outlier Detection Example using IQR Method')  # add title\n",
    "    plt.grid(True)  # show grid\n",
    "\n",
    "    plt.show()  # show the completed plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6285c3b8",
   "metadata": {},
   "source": [
    "Using the function `plot_outliers_iqr`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b802ebc0-68c8-4bf6-a35f-ab35a9722ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot data with outliers highlighted and bound lines\n",
    "plot_outliers_iqr(data, outliers_iqr, lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb66ecc1-5a09-4b2a-900a-7864d09e9421",
   "metadata": {},
   "source": [
    "Several points have been identified using this method. \n",
    "To remove them from the data we are working with we can filter the data array using Boolean indexing. <br>\n",
    "`~outliers_iqr` negates the Boolean array `outliers_iqr`, so it selects only the elements of data that are not identified as outliers (ie: Are not True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f41a15-8cf7-4ff3-a3a5-1110a171ea8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove outliers from the dataset\n",
    "cleaned_data = data[~outliers_iqr]\n",
    "\n",
    "print(\"Original data shape:\", data.shape)\n",
    "print(\"Cleaned data shape:\", cleaned_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436d50ce-3780-4c46-8065-0aaf65267b01",
   "metadata": {},
   "source": [
    "#### Z-score (standard score) method:\n",
    "\n",
    "A z-score represents the number of standard deviations a data point is from the mean of a dataset.\n",
    "Mathematically, the z-score of a data point $x$ in a dataset with mean $Î¼$ and standard deviation \n",
    "$Ïƒ$ is calculated as:<br>\n",
    "> $Z=  \\frac{xâˆ’Î¼}{Ïƒ}$\n",
    "<br>\n",
    "\n",
    "A z-score of 0 means the data point is exactly at the mean, a positive z-score means the data point is above the mean, and a negative z-score means the data point is below the mean.\n",
    "\n",
    "\n",
    "Generally, instances with a z-score over 3 are chosen as outliers.\n",
    "This concept refers to data points that are located at 3 standard deviations from the mean of the dataset.\n",
    "It's often used as a threshold for identifying outliers, especially in normally distributed datasets, where approximately 99.7% of the data falls within 3 standard deviations of the mean (assuming a normal distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24db522",
   "metadata": {},
   "source": [
    "Letâ€™s generate the dataset we are going to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab8fd5-46cf-4881-9cfe-f6eca94b5a4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = generate_demo_dataset4() # original dataframe\n",
    "print(df,\"\\n\\n\") # print dataframe with two carriage returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f12e06",
   "metadata": {},
   "source": [
    "And apply the above formula to calculate the z-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af32b68-3734-498e-91e2-939fd794c406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate z-scores\n",
    "z_scores = (data - np.mean(data)) / np.std(data)\n",
    "print(z_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856e810a",
   "metadata": {},
   "source": [
    "Now we have calculated the z-scores, lets create an array of outliers (True/False) that are greater than the general threshold of 3 standard deviations from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6437e50f-ef36-4f26-b336-3409af2934d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define threshold for outlier detection\n",
    "threshold = 3\n",
    "\n",
    "# Identify outliers\n",
    "outliers = np.abs(z_scores) > threshold\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993d0f17-e4ce-481c-8370-b6afdc20a8ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can put all this into a function to make the calculation easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7972499-39cd-45ba-a1e5-99278076043a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def identify_outliers_zscore(data, threshold=3):\n",
    "    \"\"\"\n",
    "    Identify outliers in a dataset using z-scores.\n",
    "\n",
    "    Args:\n",
    "    data (numpy.ndarray or pandas.Series): The data for which outliers are to be identified.\n",
    "    threshold (float, optional): The threshold value to determine outliers. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A boolean array indicating outliers in the data.\n",
    "    \"\"\"\n",
    "    # Calculate z-scores\n",
    "    z_scores = (data - np.mean(data)) / np.std(data)\n",
    "    \n",
    "    # Identify outliers\n",
    "    outliers = np.abs(z_scores) > threshold\n",
    "    \n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7563b9d9-b78d-4026-ae62-9296ea11a38b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the function\n",
    "outliers = identify_outliers_zscore(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de85a3",
   "metadata": {},
   "source": [
    "This function will plot the results, generate this function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2740e3ee-3a3a-49aa-8b49-b3a38fa7839d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_outliers_zscore(data, outliers, threshold=3):\n",
    "    \"\"\"\n",
    "    Plot the data with identified outliers using the z-score method.\n",
    "\n",
    "    Args:\n",
    "    data (numpy.ndarray or pandas.Series): The original data to be plotted.\n",
    "    outliers (numpy.ndarray): A boolean array indicating outliers in the data.\n",
    "    threshold (float, optional): The threshold value to determine outliers. Defaults to 3.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))  # create a blank figure to plot on\n",
    "\n",
    "    plt.plot(data, label='Data')  # plot the data\n",
    "    plt.plot(np.where(outliers)[0], data[outliers], 'ro', label='Outliers')  # highlight the outliers\n",
    "\n",
    "    plt.axhline(np.mean(data), color='green', linestyle='-', label='Mean')  # mean\n",
    "    # plt.axhline(np.median(data), color='purple', linestyle='-', label='Median')  # median\n",
    "    \n",
    "    # add lower threshold line\n",
    "    plt.axhline(np.mean(data) - (threshold * np.std(data)), color='gray', linestyle='--', label='Lower Threshold')  \n",
    "     # add upper threshold line\n",
    "    plt.axhline(np.mean(data) + (threshold * np.std(data)), color='gray', linestyle='--', label='Upper Threshold') \n",
    "\n",
    "    plt.legend()  # add legend\n",
    "    plt.xlabel('Index')  # add x label\n",
    "    plt.ylabel('Value')  # add y label\n",
    "    plt.title('Outlier Detection Example using z-Score (standard score) Method')  # add title\n",
    "    plt.grid(True)  # show grid\n",
    "    plt.show()  # show the completed plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1445fa3-53ba-4c9c-b29b-3186bef082dd",
   "metadata": {},
   "source": [
    "... and now use `plot_outliers_zscore` to plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225cd6b4-2e1d-4c4f-ba28-8cfe3df6dc2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot data with outliers highlighted\n",
    "plot_outliers_zscore(data, outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59bf2d-5014-47df-862f-8a7654ada84a",
   "metadata": {},
   "source": [
    "<b>Practical Task 1.4</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9680a-d40e-4ad8-b5c0-eef9624b3b9a",
   "metadata": {},
   "source": [
    "From the following dataset identify and plot the outliers for cholesterol levels using both the methods we have covered:<br>\n",
    "Use the supplied functions to identify the outliers and plot the results.\n",
    " - IQR Method: <br>\n",
    "     Functions: `identify_outliers_iqr` and `plot_outliers_iqr`<br>\n",
    "     <br>\n",
    " - Z-Score Method:<br> \n",
    " Functions: `identify_outliers_zscore` and `plot_outliers_zscore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad9254-b557-42fe-8191-b6e58f2c74cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate_practical_dataset4\n",
    "# import numpy as np\n",
    "\n",
    "def generate_practical_dataset4(num_patients=1000):\n",
    "    \"\"\"\n",
    "    Generate random healthcare data for a specified number of patients.\n",
    "\n",
    "    Parameters:\n",
    "    - num_patients (int): Number of patients for which healthcare data is generated. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "    - ndarray: A 2D NumPy array containing healthcare data with the following columns:\n",
    "               - Age of patients\n",
    "               - Cholesterol levels in mg/dL\n",
    "               - Blood pressure in mmHg\n",
    "               - Body Mass Index (BMI)\n",
    "    \"\"\"\n",
    "    # Generate random healthcare data\n",
    "    age = np.random.randint(18, 90, num_patients)  # Age of patients\n",
    "    cholesterol = np.random.normal(200, 30, num_patients)  # Cholesterol levels in mg/dL\n",
    "    blood_pressure = np.random.randint(90, 180, num_patients)  # Blood pressure in mmHg\n",
    "    bmi = np.random.normal(25, 4, num_patients)  # Body Mass Index (BMI)\n",
    "\n",
    "    # Stack arrays horizontally to create a single 2D array\n",
    "    healthcare_data = np.column_stack((age, cholesterol, blood_pressure, bmi))\n",
    "\n",
    "    return healthcare_data\n",
    "\n",
    "# Call the function and print first few rows of the healthcare data\n",
    "healthcare_data = generate_practical_dataset4()\n",
    "print(\"Sample healthcare data (first 5 rows):\")\n",
    "print(healthcare_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d950a0-43ae-4feb-89de-fd569d890a00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print the first 50 rows of the cholesterol column\n",
    "print(healthcare_data[:50, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fc7fc9-3ccb-45d5-bbb9-1178bfda4a7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# healthcare_data = generate_practical_dataset4()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a41fcf-b838-4ceb-b746-70a025d2f188",
   "metadata": {},
   "source": [
    "### Dealing with Target Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2ed139-dceb-4ede-9be6-692a9359ae89",
   "metadata": {},
   "source": [
    "Before diving into this section, we first need to understand the terms 'Target' and 'Features'.\n",
    "\n",
    "Target(s) - used to describe the column(s) you are trying to predict in your machine learning model. <br>\n",
    "Features - Are all the other columns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63e3d5d-70f6-4f7e-a092-c5fff07fe87f",
   "metadata": {},
   "source": [
    "So, target imbalance is when our target column, also known as the target variable, has fewer instances in the data of the thing we are trying to predict.\n",
    "\n",
    "#### Why do we need to address target imbalance?\n",
    "\n",
    "Addressing target imbalance is crucial in many machine learning tasks, particularly in classification problems, because it ensures that the model doesn't become biased towards the majority class. <br>\n",
    "When the classes in your dataset are imbalanced, meaning some classes have significantly more samples than others, the model may learn to simply predict the majority class for most instances.\n",
    "\n",
    "When assessing target imbalance, there isn't a fixed threshold that universally determines whether there's an imbalance or not. <br> However, a common rule of thumb to determine whether a class imbalance is significant, is if one class represents less than 10% to 20% of the total dataset.\n",
    "\n",
    "A real world example of this would be detecting credit card fraud transactions, or in healthcare, \"did not attend\" (DNA) rates in outpatient appointments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1565c013-82a5-4deb-94f0-a77fe60cf6cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "Letâ€™s take a look at some ways to tackle target imbalance.\n",
    "\n",
    "Generate the below patient data where the target is the `Disease` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83dcf95-c1d5-4bc4-8bf3-fa7d6b8bfd8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate_demo_dataset5\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "def generate_demo_dataset5():\n",
    "    \"\"\"\n",
    "    Generate a synthetic demographic dataset.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the generated dataset with columns:\n",
    "        - 'Age': Age of the individuals.\n",
    "        - 'Gender': Gender of the individuals (1) male or (2) female.\n",
    "        - 'Blood Pressure': Blood pressure of the individuals.\n",
    "        - 'Disease': Target variable indicating the presence (1) or absence (0) of a disease.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Features\n",
    "    age = np.random.randint(20, 80, size=1000)\n",
    "    gender = np.random.choice([1, 2], size=1000)\n",
    "    blood_pressure = np.random.randint(90, 180, size=1000)\n",
    "\n",
    "    # Target variable\n",
    "    disease = np.random.choice([0, 1], size=1000, p=[0.9, 0.1])\n",
    "\n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'Age': age,\n",
    "        'Gender': gender,\n",
    "        'Blood Pressure': blood_pressure,\n",
    "        'Disease': disease\n",
    "    })\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "df = generate_demo_dataset5()\n",
    "print(df)\n",
    "\n",
    "# Display the shape of dataframe\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7a9f49-2602-4b26-a75a-38c15a0812ad",
   "metadata": {},
   "source": [
    "Inspect the 'Disease' target variable to identify if there is a significant imbalance. <br>\n",
    "Tip: Note these numbers down as it will help you with checking the following methods we are about to cover!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77183a84-93e1-4b93-af70-7322294ee241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Disease'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044c436-92f8-44d9-bc6d-e0f41c9ad5ef",
   "metadata": {},
   "source": [
    "This can be quickly plotted to give a quick visual representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57236bb3-f048-4caf-b1ba-ab43f862d90d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Disease'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed1d19",
   "metadata": {},
   "source": [
    "To summarise before moving on. Most records, in this dataset, are where the Disease target variable is False - i.e.: No Disease. The remaining records represent the minority class, where the Disease variable is True. As the aim is to predict when Disease = True we need to address the imbalance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeecac3-6f1c-42d2-87be-fafb2e2b949d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SMOTE (Synthetic Minority Over-sampling Technique):\n",
    "\n",
    "One way to dealing with target imbalance is a methodology called SMOTE, which stands for Synthetic Minority Over-sampling TEchnique. \n",
    "\n",
    "The way that SMOTE works is that for each minority class instance in the data, SMOTE will find it's `k` nearest neighbours (where `k` is a user specified number) in the feature space. <br>\n",
    "From this it then generates synthetic samples by creating new instances along the line segments connecting the minority class instance to its nearest neighbours.<br>\n",
    "These synthetic samples are then added to the original dataset, which effectively increases the number of the minority class instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bdb26b",
   "metadata": {},
   "source": [
    "To demonstrate this we are going to use part of the `imblearn` library. <br>\n",
    "The imbalanced-learn library (abbreviated as imblearn) is a Python library specifically designed to address the problem of class imbalance in machine learning datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf3ec0f",
   "metadata": {},
   "source": [
    "> Note: We have already imported parts of the `imblearn` library at the start of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88183fc",
   "metadata": {},
   "source": [
    "For SMOTE we use the following library:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d611c7",
   "metadata": {},
   "source": [
    "```python\n",
    "# import SMOTE from imblearn.over_sampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f1122",
   "metadata": {},
   "source": [
    "First, we need to separate the features and target to X and y respectively - this is a common standard notation for features and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d464d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = df.drop('Disease', axis=1) # drop target variable leaving the remaining features\n",
    "y = df['Disease'] # just the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5412dae",
   "metadata": {},
   "source": [
    "Task: <br>\n",
    "* Check the number of rows and columns in X (features) and y (target variable) <br>\n",
    "* And then also plot the values of the target variable y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd953b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0908ac88",
   "metadata": {},
   "source": [
    "We now have our features and target separated, so we can now start with instantiating smote (create an instance of the class, in this case and instance of SMOTE). Then use it to 'resample' the dataset for the X and y.\n",
    "\n",
    "To do this we use:\n",
    "`SMOTE(random_state=42)` which creates an instance of the SMOTE algorithm with a specific random state (here 42). The random state is an arbitrary choice, and you could use any integer value. The important aspect is to keep it consistent across runs if reproducibility is desired.  \n",
    "\n",
    "> Geek Alert: You will see 42 often used in notebooks from other data scientists. The use of the number 42 as the random_state parameter in machine learning is actually a reference to the science fiction series \"The Hitchhiker's Guide to the Galaxy\" by Douglas Adams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c264fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SMOTE\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8038db",
   "metadata": {},
   "source": [
    "Now use this to resample your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cf2aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the dataset\n",
    "X_resampled_smote, y_resampled_smote = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce0b03",
   "metadata": {},
   "source": [
    "Check the number of rows and columns in `X_resampled` and `y_resampled`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc7e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9fe633",
   "metadata": {},
   "source": [
    "So, where Disease = False we had x records, this being the majority class, SMOTE will take the minority class, of Disease = True with y records, and increase this to x records to make 'the total number of records for Disease = True' balanced with 'the total records for Disease = False'. <br>\n",
    "Therefore, the total number of resulting records will be double the majority class. <br>\n",
    "\n",
    "Look at the resampled target variable value counts to confirm this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984f2056-36d8-47fa-9015-b1a391c8f159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_resampled_smote.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313e6cc3",
   "metadata": {},
   "source": [
    "Let's quickly plot the before and after using the below function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ac0a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_before_and_after_resampling(y, y_resampled, label):\n",
    "    \"\"\"\n",
    "    Plot bar plots before and after resampling.\n",
    "\n",
    "    Args:\n",
    "        y (pandas.Series): Original target variable.\n",
    "        y_resampled (pandas.Series): Resampled target variable.\n",
    "        label (str): Label to be used in the plot title for the resampled data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Create a figure and axis object\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "    # Plot the first bar plot for y\n",
    "    y.value_counts().plot(kind='bar', ax=axs[0])\n",
    "    axs[0].set_title('y')\n",
    "    axs[0].set_xlabel('Disease')\n",
    "    axs[0].set_ylabel('Number of Records')\n",
    "\n",
    "    # Plot the first bar plot for y_resampled\n",
    "    y_resampled.value_counts().plot(kind='bar', ax=axs[1])\n",
    "    axs[1].set_title(f'y_resampled using {label}')\n",
    "    axs[1].set_xlabel('Disease')\n",
    "    axs[1].set_ylabel('Number of Records')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b367d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_before_and_after_resampling(y,y_resampled_smote,\"SMOTE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f3f3a7-17dd-4310-994d-c18a574864c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Random oversampling:\n",
    "\n",
    "RandomOverSampler simply duplicates some samples from the minority class to balance the dataset. It randomly selects instances from the minority class and replicates them until the dataset is balanced.\n",
    "\n",
    "Generate the dataset again if required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0fb042",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_demo_dataset5()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64682206",
   "metadata": {},
   "source": [
    "For random over sampling we use the following library:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd2daf4",
   "metadata": {},
   "source": [
    "```python\n",
    "# import RandomOverSampler from imblearn.over_sampling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91682e4",
   "metadata": {},
   "source": [
    "We have previously already separated the features and the target variable, so we don't need to repeat this. As a reminder the code was:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c5928d",
   "metadata": {},
   "source": [
    "```python\n",
    "# Separate features and target variable\n",
    "X = df.drop('Disease', axis=1)\n",
    "y = df['Disease']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b57baa0",
   "metadata": {},
   "source": [
    "Now we create an instance of the RandomOverSampler algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fc38e5-4018-4153-8cb0-0b0df51641e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e8192",
   "metadata": {},
   "source": [
    "Now use this to resample your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe59de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the dataset\n",
    "X_resampled_ros, y_resampled_ros = ros.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7e4a01",
   "metadata": {},
   "source": [
    "And plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34310f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_before_and_after_resampling(y,y_resampled_ros,\"RandomOverSampler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b0fd8",
   "metadata": {},
   "source": [
    "What do you notice comparing these results to SMOTE?\n",
    "\n",
    "The results should be the same but the technique that the algorithms use is very different, but both are over sampling algorithms.\n",
    "\n",
    "Use SMOTE when the minority class is densely packed or when there is overlapping with the majority class. SMOTE synthesises new minority class samples along the lines connecting existing minority class samples, effectively creating synthetic examples within the feature space.\n",
    "\n",
    "Use RandomOverSampler when the minority class is spread out and there is less risk of creating overlapping or synthetic examples that might not represent the true distribution of the minority class. RandomOverSampler simply duplicates minority class samples, maintaining the original distribution.\n",
    "\n",
    "In both cases over sampling is best to use on smaller datasets, as potentially a lot of extra records will be created to meet the balance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f46a89e",
   "metadata": {},
   "source": [
    "To help with identifying whether the minority class is densely packed, overlapping with the majority class or being spread out, use a `seaborn` pair plot to quickly visualise patterns. <br>\n",
    "* `Seaborn` is a visualisation library which is imported as `import seaborn as sns`.<br>\n",
    "* A pair plot, also known as a scatterplot matrix, is a type of visualisation that allows you to explore relationships between pairs of variables in a dataset. It's particularly useful for datasets with multiple variables, enabling you to quickly identify patterns, correlations, and potential insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deb1283",
   "metadata": {},
   "source": [
    "Use the functions `original_target_variable_pair_plot` and `resampled_target_variable_pair_plot` to compare the results of\n",
    "* The original data\n",
    "* Smote resampled data\n",
    "* Random oversampling resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523c74a6",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "def original_target_variable_pair_plot(df):\n",
    "    \"\"\"\n",
    "    Generate a pair plot to visualise the distribution of features by disease class for original data.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the original data.\n",
    "    \"\"\"\n",
    "    # Visualising the distribution of features by disease class\n",
    "    sns.pairplot(df, hue='Disease', height=2)\n",
    "    # Add a title\n",
    "    plt.suptitle('Pair Plot of Features by Disease Class - Original Data', y=1.05)\n",
    "    plt.show()\n",
    "\n",
    "    # Calculating the average distance between minority class samples\n",
    "    minority_samples = df[df['Disease'] == 1][['Age', 'Blood Pressure']]\n",
    "    mean_distance = np.mean(np.linalg.norm(minority_samples - minority_samples.mean(axis=0), axis=1))\n",
    "    print(\"Average distance between minority class samples:\", mean_distance)\n",
    "    \n",
    "    return\n",
    "\n",
    "def resampled_target_variable_pair_plot(X_resampled, y_resampled, label):\n",
    "    \"\"\"\n",
    "    Generate a pair plot to visualise the distribution of features by disease class after resampling.\n",
    "\n",
    "    Parameters:\n",
    "    X_resampled (array-like): The resampled features.\n",
    "    y_resampled (array-like): The resampled target variable.\n",
    "    label (str): The label indicating the type of resampling performed.\n",
    "    \"\"\"\n",
    "    # Concatenate the resampled features and target variable\n",
    "    df_resampled = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), \n",
    "                              pd.DataFrame(y_resampled, columns=['Disease'])], axis=1)\n",
    "\n",
    "    # Visualising the distribution of features by disease class after resample\n",
    "    sns.pairplot(df_resampled, hue='Disease', height=2)\n",
    "    plt.suptitle(f'Pair Plot of Features by Disease Class - {label} Resampled Data', y=1.05)\n",
    "    plt.show()\n",
    "\n",
    "    # Calculating the average distance between minority class samples\n",
    "    minority_samples = df_resampled[df_resampled['Disease'] == 1][['Age', 'Blood Pressure']]\n",
    "    mean_distance = np.mean(np.linalg.norm(minority_samples - minority_samples.mean(axis=0), axis=1))\n",
    "    print(\"Average distance between minority class samples:\", mean_distance)\n",
    "\n",
    "    return   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf25b5d",
   "metadata": {},
   "source": [
    "Run the functions and review the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data\n",
    "original_target_variable_pair_plot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61387b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote resampled data\n",
    "resampled_target_variable_pair_plot(X_resampled_smote, y_resampled_smote, \"smote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc829a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random over sampled resampled data\n",
    "resampled_target_variable_pair_plot(X_resampled_ros, y_resampled_ros, \"random over sampler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ffa37-3aaa-4922-b1cc-7775dc0ab6df",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Random Under-sampling:\n",
    "\n",
    "Random under-sampling can be effective when the dataset is very large and the computational resources are limited. However, the trade of is that it comes with the risk of losing potentially valuable information from the majority class.\n",
    "\n",
    "Generate the dataset again if required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d0250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_demo_dataset5()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1013ae0",
   "metadata": {},
   "source": [
    "For random over-sampling we use the following library:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751b086",
   "metadata": {},
   "source": [
    "```python\n",
    "# import RandomUnderSampler from imblearn.under_sampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a880a297",
   "metadata": {},
   "source": [
    "You should be familiar with the steps to carry out resampling as they are the same as before just with a new algorithm `RandomUnderSampler`. <br>\n",
    "We will do this in one code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2092b9ae-8f7a-4dc0-9729-4a4e5f1928a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = df.drop('Disease', axis=1)\n",
    "y = df['Disease']\n",
    "\n",
    "# Instantiate RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Resample the dataset\n",
    "X_resampled_rus, y_resampled_rus = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e5e07",
   "metadata": {},
   "source": [
    "And go straight to plotting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066e15ac-8a2d-4631-bf94-fb1e099fc185",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_before_and_after_resampling(y,y_resampled_rus,\"RandomUnderSampler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332f618",
   "metadata": {},
   "source": [
    "This should show what you expected that the majority class (Disease = False) has been randomly reduced to the same number of records of the minority class (Disease = True)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30708ece-14f9-46bb-a4f8-4a7366d1c3db",
   "metadata": {
    "tags": []
   },
   "source": [
    "<b>Practical Task 1.5</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e4dc2-c656-459a-9640-b77d22aeb5d8",
   "metadata": {},
   "source": [
    "Using one of the above methods look to address the target imbalance of this new dataset.\n",
    "* The target variable is: Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0a1b21-de67-4c74-9b97-0014f0c8fd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_practical_dataset5\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "def generate_practical_dataset5():\n",
    "    \"\"\"\n",
    "    Generate a synthetic healthcare-related dataset with imbalanced classes.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the generated dataset with columns:\n",
    "        - 'Age': Age of the patients.\n",
    "        - 'Gender': Gender of the patients (1 for male, 2 for female).\n",
    "        - 'Blood Pressure': Blood pressure of the patients.\n",
    "        - 'Cholesterol': Cholesterol level of the patients.\n",
    "        - 'Diabetes': Target variable indicating the presence (1) or absence (0) of diabetes.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Features\n",
    "    age = np.random.randint(20, 80, size=1000)\n",
    "    gender = np.random.choice([1, 2], size=1000)\n",
    "    blood_pressure = np.random.randint(90, 180, size=1000)\n",
    "    cholesterol = np.random.randint(120, 300, size=1000)\n",
    "\n",
    "    # Target variable\n",
    "    # Introduce class imbalance (90% negative class, 10% positive class)\n",
    "    diabetes = np.random.choice([0, 1], size=1000, p=[0.9, 0.1])\n",
    "\n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'Age': age,\n",
    "        'Gender': gender,\n",
    "        'Blood Pressure': blood_pressure,\n",
    "        'Cholesterol': cholesterol,\n",
    "        'Diabetes': diabetes\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate the synthetic healthcare dataset\n",
    "healthcare_df = generate_practical_dataset5()\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(healthcare_df.head())\n",
    "\n",
    "# Display the shape of the dataset\n",
    "print(\"Shape of the dataset:\", healthcare_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e036145",
   "metadata": {},
   "source": [
    "1. Check the target variable and confirm the imbalance. You can also plot this if you wish using `.plot.bar()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ab8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc83b181",
   "metadata": {},
   "source": [
    "2. Separate the features and target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006637ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870becc4",
   "metadata": {},
   "source": [
    "3. Pick a method for dealing with the imbalance and instantiate the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5837a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d4a840",
   "metadata": {},
   "source": [
    "4. Resample the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f4d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f815fa10",
   "metadata": {},
   "source": [
    "5. Plot the results of before and after the resampling method using function: `plot_before_and_after_resampling`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66be819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d559a801-65d7-43ec-9ad0-916e41f8343c",
   "metadata": {},
   "source": [
    "### Chapter Summary\n",
    "\n",
    "Well done on reaching the end of this chapter! <br>\n",
    "Just to recap what we have learnt when is comes to cleaning and preparing our data, you should now feel familiar with:\n",
    " - Checking and converting data types. <br>\n",
    " - Handling missing values by removing specific rows or columns based on missing values being present and looked at filling in missing values using statistical values from the dataset. <br>\n",
    " - Removing duplicate records and duplicates across specific columns. <br>\n",
    " - Identifying and handling outliers in the data.\n",
    " - Dealing with target imbalance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
