{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518f7e1e-c7c0-4186-9d24-8d4ae052f006",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5829d990-ce5c-4269-90c0-17e8fa287a17",
   "metadata": {},
   "source": [
    "## Feature Engineering, Scaling and Transforming\n",
    "\n",
    "The next step in data preprocessing involves feature engineering, where categorical variables undergo conversion into numerical values, new features are generated, and various data transformations are applied. Scaling and transforming is carried out on the numerical features and either adjusting the range of values or using mathematical operations or functions to adjust the values. <br>\n",
    "These steps collectively aim to ensure the data's robustness and suitability for efficient and reliable model execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8d6cc1-e262-4e4b-a256-d0433132616b",
   "metadata": {},
   "source": [
    "Within this chapter the following topics will be covered: <br>\n",
    "<b>Feature Engineering:</b>\n",
    "* Creating New Features.\n",
    "    * Bin Numeric Features.\n",
    "    * Group Features.\n",
    "* Encoding Categorical Variables.\n",
    "    * Label Encoding.\n",
    "    * One-Hot Encoder.\n",
    "    * Ordinal Encoding.\n",
    "* Combine Rare Levels / Cardinal Encoding.\n",
    "* Removing Multicollinearity.\n",
    "\n",
    "<b>Scaling and Transforming:</b>\n",
    "* Normalising or Scaling the Data.\n",
    "* Transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d84be0b-f1f7-4ad3-94c2-6e243e12d8b0",
   "metadata": {},
   "source": [
    "Import the following libraries for this chapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0670b80-d8f8-4a44-b612-7d1dc4252a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Encoding categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Normalise data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scaling data\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700109d8",
   "metadata": {},
   "source": [
    "### Creating New Features\n",
    "\n",
    "By creating new features, you can provide additional information to the machine learning model. <br>\n",
    "New features can capture relationships and patterns in the data that the original features might not fully represent and can also help make the data more interpretable. <br> \n",
    "\n",
    "As we have already seen and covered how missing data can be a challenge in machine learning. By creating new features, based on existing features, we can help mitigate the impact of missing values. <br>\n",
    "\n",
    "We are going to look at two types of feature creation, binning numeric features and group features in this next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e13bf08",
   "metadata": {},
   "source": [
    "#### Bin Numeric Features\n",
    "Binning numeric features can be used to separate continuous numerical variables into a set of predefined bins or intervals. <br>\n",
    "This process involves partitioning the range of the variable into discrete segments, or bins, and assigning each data point to its corresponding bin. <br>\n",
    "\n",
    "Generate the below dataset from the function `generate_fe_demo_dataset1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d1101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo Dataset 1\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "def generate_fe_demo_dataset1():\n",
    "    \"\"\"\n",
    "    Generate a synthetic healthcare dataset for feature engineering.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing synthetic healthcare data with the following columns:\n",
    "            - PatientID: Unique identifier for each patient.\n",
    "            - Ward: Ward in which the patient is admitted.\n",
    "            - Age: Age of the patient.\n",
    "            - Gender: Gender of the patient (Male/Female).\n",
    "            - BMI: Body Mass Index of the patient.\n",
    "            - Systolic_BP: Systolic Blood Pressure of the patient.\n",
    "            - Diastolic_BP: Diastolic Blood Pressure of the patient.\n",
    "            - Cholesterol: Cholesterol level of the patient.\n",
    "            - Diabetes: Whether the patient has diabetes (Y/N).\n",
    "            - Smoker: Whether the patient is a smoker (Y/N).\n",
    "            - Cigarettes_per_day: Number of cigarettes smoked per day for smokers (0 for non-smokers).\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'patient_id': range(5001, 5101),\n",
    "        'ward': np.random.choice([\n",
    "            \"Medical Ward 1\", \"Medical Ward 2\", \"Medical Ward 3\", \"Medical Ward 4\", \"Medical Ward 5\", \n",
    "            \"Medical Ward 6\", \"Medical Ward 7\", \"Medical Ward 8\", \"Medical Ward 9\", \"Medical Ward 10\",\n",
    "            \"Medical Ward 11\", \"Medical Ward 12\", \"Medical Ward 13\", \"Medical Ward 14\", \"Medical Ward 15\", \n",
    "            \"Surgical Ward 1\", \"Surgical Ward 2\", \"Surgical Ward 3\", \"Surgical Ward 4\", \"Surgical Ward 5\", \n",
    "            \"Surgical Ward 6\", \"Surgical Ward 7\", \"Surgical Ward 8\"\n",
    "        ], size=100),\n",
    "        'age': np.random.randint(20, 80, size=100),\n",
    "        'gender': np.random.choice(['Male', 'Female'], size=100),\n",
    "        'bmi': np.random.uniform(18.5, 35.0, size=100),\n",
    "        'systolic_bp': [\n",
    "            172, 135, 86, 147, 123, 99, 142, 129, 158, 123, 115, 100, 120, 107, 124, 101, 111, 107, 94, 129, \n",
    "            149, 144, 99, 91, 121, 114, 134, 145, 156, 109, 138, 87, 178, 104, 148, 146, 105, 117, 120, 138, \n",
    "            130, 126, 162, 118, 146, 80, 135, 139, 163, 133, 148, 134, 111, 180, 135, 131, 125, 101, 93, 129, \n",
    "            149, 117, 155, 214, 124, 114, 132, 139, 122, 118, 162, 128, 128, 129, 99, 131, 130, 102, 108, 114, \n",
    "            91, 101, 133, 139, 146, 125, 121, 107, 146, 151, 130, 118, 118, 80, 136, 123, 137, 125, 106, 120\n",
    "        ],\n",
    "        'diastolic_bp': [\n",
    "            81, 102, 41, 79, 68, 58, 61, 65, 140, 58, 79, 64, 63, 82, 83, 64, 71, 81, 66, 71, \n",
    "            73, 94, 58, 49, 83, 69, 69, 57, 68, 76, 99, 73, 82, 71, 64, 88, 71, 74, 79, 54, \n",
    "            113, 63, 70, 72, 54, 55, 55, 78, 81, 75, 72, 75, 61, 99, 67, 86, 85, 70, 57, 85, 80, \n",
    "            68, 111, 84, 60, 67, 89, 68, 68, 70, 95, 85, 76, 56, 58, 70, 66, 73, 60, 66, 66, 65, \n",
    "            84, 61, 109, 58, 79, 86, 65, 90, 85, 70, 80, 52, 57, 95, 78, 64, 59, 53\n",
    "        ],\n",
    "        'cholesterol': np.random.randint(120, 300, size=100),\n",
    "        'diabetes': np.random.choice(['Y', 'N'], size=100, p=[0.2, 0.8]),\n",
    "        'smoker': np.random.choice(['Y', 'N'], size=100, p=[0.4, 0.6]),\n",
    "    }\n",
    "\n",
    "    # Conditionally generate cigarettes_per_day based on smoker status\n",
    "    data['cigarettes_per_day'] = [np.random.randint(1, 30) if smoker == 'Y' else 0 for smoker in data['smoker']]\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65021e9d",
   "metadata": {},
   "source": [
    "Now use the generated function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6317f158",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_fe_demo_dataset1()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955fbac7",
   "metadata": {},
   "source": [
    "From this dataset we want to create a new feature, that will be categorical, which 'bins' the age of patients into 10-year bandings.\n",
    "\n",
    "This is how it is done, but let's break this down into its components:<br>\n",
    ">`df['age_band'] = pd.cut(df['age'], bins=range(20, 90, 10), labels=[f\"{i}-{i+9}\" for i in range(20, 80, 10)])`\n",
    "\n",
    "* First off we define our new column name: `df['age_band']`. <br>\n",
    "* We use `pd.cut()` on the `df['age']` column from the dataframe. This function is used to segment and sort data values into bins. <br>It takes several arguments: <br>\n",
    "    * The first argument `df['age']` is the data to be segmented, in this case, the 'age' column.<br>\n",
    "    * `bins=range(20, 90, 10)`: This specifies the bins or intervals into which the data will be divided. <br>\n",
    "  It creates bins starting from 20 up to (but not including) 90, with a step size of 10. <br>\n",
    "  So, the bins will be [20-29], [30-39], [40-49], ..., [80-89].\n",
    "    * `labels=[f\"{i}-{i+9}\" for i in range(20, 80, 10)]`: This specifies the labels to assign to each bin in the format [lower bound - upper bound]. So will generate labels like this [20-29], [30-39], [40-49], ..., [70-79]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1eb186",
   "metadata": {},
   "source": [
    "Letâ€™s apply this to our dataframe and look at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f134ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create age bands\n",
    "df['age_band'] = pd.cut(df['age'], bins=range(20, 90, 10), labels=[f\"{i}-{i+9}\" for i in range(20, 80, 10)])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e08176",
   "metadata": {},
   "source": [
    "<b>Practical Task 2.1<b/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca0b31",
   "metadata": {},
   "source": [
    "We now want to categorise the `cholesterol` column based on the below values.\n",
    "Following what we have just learnt create a new feature named 'cholesterol_category'.\n",
    "\n",
    "* < 200 - Healthy Level\n",
    "* 200 - 239 - At Risk\n",
    "* => 240 - Dangerous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f05ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670c04a9",
   "metadata": {},
   "source": [
    "#### Group Features\n",
    "Grouping features involves aggregating or combining related features into a single feature. <br>\n",
    "This is where multiple related features are combined to create new, more informative features. It can also involve grouping similar features together to represent a broader aspect of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc85f65e",
   "metadata": {},
   "source": [
    "From this we will create a new group feature, combining the systolic_bp and diastolic_bp, to categorise the blood pressure results.\n",
    "\n",
    "For this a function has been defined. Look at the function and apply it to a new column `bp_category`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69610079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorise_blood_pressure(systolic, diastolic):\n",
    "    \"\"\"\n",
    "    Categorises blood pressure based on systolic and diastolic readings.\n",
    "\n",
    "    Parameters:\n",
    "        systolic (int): The systolic blood pressure reading.\n",
    "        diastolic (int): The diastolic blood pressure reading.\n",
    "\n",
    "    Returns:\n",
    "        str: A string indicating the blood pressure category.\n",
    "            Possible categories:\n",
    "                - \"Low\" for systolic <= 90 and diastolic <= 60\n",
    "                - \"Ideal\" for systolic <= 120 and diastolic <= 80\n",
    "                - \"Pre-high\" for systolic <= 135 and diastolic <= 85\n",
    "                - \"High\" for all other cases\n",
    "    \"\"\"\n",
    "    if systolic <= 90 and diastolic <= 60:\n",
    "        return \"Low\"\n",
    "    elif systolic <= 120 and diastolic <= 80:\n",
    "        return \"Ideal\"\n",
    "    elif systolic <= 135 and diastolic <= 85:\n",
    "        return \"Pre-high\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d952ce",
   "metadata": {},
   "source": [
    "We can apply the function to the dataframe like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2b0148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 'blood_pressure_category' column\n",
    "df['bp_category'] = df.apply(lambda row: categorise_blood_pressure(row['systolic_bp'], row['diastolic_bp']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e35f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655067f",
   "metadata": {},
   "source": [
    "### Encoding Categorical Variables\n",
    "\n",
    "Categorical data cannot typically be directly handled by machine learning algorithms, as most algorithms are designed to operate with numerical data only. Therefore, before categorical features can be used as inputs to machine learning algorithms, they must be encoded as numerical values.\n",
    "\n",
    "We are going to look at the various ways in which categorical variables can be encoded and what you need to consider ensuring you select the correct method of encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04808cef",
   "metadata": {},
   "source": [
    "We are going to continue using the same dataset from the last section, as we are going to be working with the newly created features. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c1c4fc-294e-4bc6-9bae-01890c217569",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Label Encoding\n",
    "Label encoding is a technique used in data preprocessing where categorical variables are converted into numerical values. This process assigns a unique numerical label to each category within a feature.\n",
    "\n",
    "To carry out label encoding we need to import the following library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9871b5fb",
   "metadata": {},
   "source": [
    "```python\n",
    "# import LabelEncoder from sklearn.preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6cc713",
   "metadata": {},
   "source": [
    "First, we need to instantiate an instance of the LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dfc09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate LabelEncoder\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f3b10c",
   "metadata": {},
   "source": [
    "Next, we are going to apply the encoder to the `gender` column. <br>\n",
    "\n",
    "It is best to overwrite the existing column for this as:\n",
    "* Overwriting the original field saves memory since you're not storing redundant information.\n",
    "* There's less complexity in the dataset since you're not managing multiple versions of the same information.\n",
    "* With only one version of the column, there's no confusion about which version to use.\n",
    "\n",
    "To apply the encoder we use `.fit_transform` on the `gender` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825fe281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode the 'gender' column\n",
    "df['gender'] = label_encoder.fit_transform(df['gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a39992a",
   "metadata": {},
   "source": [
    "Look at the dataframe and notice what it has changed in the `gender` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9fe2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c6758e",
   "metadata": {},
   "source": [
    "We only have two different values in this column. At some point we may wish to 'decode' back to the original values, especially if there are many values that were encoded.\n",
    "To get back to the original values we use `.inverse_transform` on the `gender` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa8c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the encoded values back to original values\n",
    "df['gender'] = label_encoder.inverse_transform(df['gender'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a84f767",
   "metadata": {},
   "source": [
    "To complete this, let's reapply the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0cfa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender'] = label_encoder.fit_transform(df['gender'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5151706",
   "metadata": {},
   "source": [
    "#### One Hot Encoding\n",
    "One hot encoding is a method used to convert categorical variables into a binary format, where each category is represented by a binary vector. In this encoding scheme, each category is assigned a unique index, and then a binary vector is created where only one element is \"hot\" (set to 1) indicating the presence of that category, while all other elements are \"cold\" (set to 0). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8362de",
   "metadata": {},
   "source": [
    "To carry out one hot encoding we can use a method in `pandas` called `.get_dummies`. <br>\n",
    "\n",
    "You can also use [`sklearn.preprocessing OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) for this which is carried out in the same steps as the `LabelEncoder` that we have just covered.\n",
    "\n",
    "From our dataset we are going to use One Hot Encoding on the `smoker` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the 'smoker' column\n",
    "smoker_dummies = pd.get_dummies(df['smoker'], prefix='smoker')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fcf923",
   "metadata": {},
   "source": [
    "The coding results are in their own dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2c643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the new dataframe\n",
    "smoker_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ad34b0",
   "metadata": {},
   "source": [
    "To include this into our dataset we just need to concatenate the two dataframes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25a84f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, smoker_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e768e3",
   "metadata": {},
   "source": [
    "We will also remove the original `smoker` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937875ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('smoker', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3896c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b76187",
   "metadata": {},
   "source": [
    "#### Ordinal Encoding\n",
    "Ordinal encoding is used to convert categorical variables into numerical values based on their order or rank. <br>\n",
    "In this method, each unique category is assigned a numerical value according to either its position in a predefined order or based on the frequency of occurrence. <br>\n",
    "Ordinal encoding is commonly used when the categorical variables have a natural order or hierarchy, such as low, medium, and high, or when numerical values can represent meaningful relationships between categories.\n",
    "\n",
    "To carry out ordinal encoding we need to import the following library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc220e82",
   "metadata": {},
   "source": [
    "```python\n",
    "# import OrdinalEncoder from sklearn.preprocessing\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6852c1ee",
   "metadata": {},
   "source": [
    "From our dataset we are going to use Ordinal Encoding on the `cholesterol_category` column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5135f757",
   "metadata": {},
   "source": [
    "Firstly, we need to define a list of the possible values, in order from lowest to highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44746e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cholesterol_category_order = ['Healthy', 'At Risk', 'Dangerous']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173be29",
   "metadata": {},
   "source": [
    "Now instantiate the encoder with the orders we have just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf0b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate OrdinalEncoder with specified categories\n",
    "ordinal_encoder = OrdinalEncoder(categories=[cholesterol_category_order])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074b6ac2",
   "metadata": {},
   "source": [
    "And apply the ordinal encoding back to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f410e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cholesterol_category'] = ordinal_encoder.fit_transform(df[['cholesterol_category']])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dae02b",
   "metadata": {},
   "source": [
    "What do you notice about the encoded values that we have just created? <br>\n",
    "Use the below cell if you need to inspect the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7275bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dd86c9",
   "metadata": {},
   "source": [
    "#### Combining Rare Levels / Cardinal Encoding\n",
    "Combining rare levels, also known as cardinal encoding, is a strategy where infrequent categories within a categorical variable are grouped together into a single category. <br>\n",
    "This approach is beneficial for <b>reducing the dimensionality</b> of the feature space and addressing issues related to overfitting caused by sparse or noisy data. <br> \n",
    "By combining rare levels, the model can focus on the most common and informative categories while simplifying the representation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d7d663",
   "metadata": {},
   "source": [
    "Using our dataset, count the number of values that we have in the `ward` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfbc40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ward'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d657d64e",
   "metadata": {},
   "source": [
    "Unless we were looking for specific patterns in the data across wards, it would be better to group the different types of wards in some way to reduce the dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4630d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column to identify if the ward is medical or surgical\n",
    "df['medical_ward'] = 1\n",
    "df.loc[df['ward'].str.contains('Surgical'), 'medical_ward'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274af078",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f094b0",
   "metadata": {},
   "source": [
    "<b>Practical Task 2.2</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f88f8a",
   "metadata": {},
   "source": [
    "Using our dataset, \n",
    "* Create a new category variable for `bmi` named `bmi_category` using the classification levels below.\n",
    "* Then use ordinal encoding using the new `bmi_category` column, but create as a new column `bmi_category_encoded` so you can compare the results easily.\n",
    "\n",
    "BMI classifcation levels:\n",
    "* A BMI of 18.4 and below is classed as underweight.\n",
    "* A BMI of 18.5 to 24.9 is classed as a healthy weight.\n",
    "* A BMI of 25 to 29.9 is classed as overweight.\n",
    "* A BMI of 30 or more is classed as obese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ef903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31438e6",
   "metadata": {},
   "source": [
    "### Removing Multicollinearity\n",
    "\n",
    "#### What is Multicollinearity?\n",
    "Multicollinearity occurs when two or more predictor variables are highly correlated with each other. In other words, it means that some independent variables are linearly dependent on others. This can cause several issues in regression analysis:\n",
    "\n",
    "* Unstable Estimates: Multicollinearity can lead to unstable estimates of the coefficients in the regression model. Small changes in the data can result in large changes in the estimated coefficients.\n",
    "* Reduced Precision: Multicollinearity inflates the standard errors of the regression coefficients, which reduces the precision of the estimates. This makes it difficult to identify the true effect of each predictor variable on the target variable.\n",
    "* Difficulty in Interpretation: High multicollinearity makes it challenging to interpret the individual effects of predictor variables on the target variable. It becomes unclear which variables are truly driving the variation in the target variable.\n",
    "\n",
    "So, removing multicollinearity involves identifying and eliminating highly correlated variables from a dataset. \n",
    "\n",
    "To help identify highly correlated variables we can carry out some quick correlation analysis with a correlation matrix.\n",
    "A correlation matrix will show all the variables and identify pairs of variables with high correlation coefficients. Variables with correlation coefficients above a certain threshold (e.g., 0.7 or 0.8) are considered highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7a9e00",
   "metadata": {},
   "source": [
    "To demonstrate this, we are going to look at this new dataset that shows some highly correlated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5cbc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "def generate_correlated_dataframe():\n",
    "    \"\"\"\n",
    "    Generate a sample DataFrame with highly correlated variables and multicollinearity.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing the following columns:\n",
    "            - X1: Independent variable 1.\n",
    "            - X2: Independent variable 2, highly correlated with X1.\n",
    "            - X3: Independent variable 3.\n",
    "            - X4: Independent variable 4.\n",
    "    \"\"\"\n",
    "    # Create sample data\n",
    "    data = {\n",
    "        'X1': np.random.rand(100),  # Independent variable 1\n",
    "        'X2': np.random.rand(100),  # Independent variable 2\n",
    "        'X3': np.random.rand(100),  # Independent variable 3\n",
    "        'X4': np.random.rand(100),  # Independent variable 4\n",
    "    }\n",
    "\n",
    "    # Create multicollinearity by adding a new variable that's highly correlated with X1\n",
    "    data['X2'] = data['X1'] + np.random.normal(0, 0.1, 100)\n",
    "\n",
    "    # Convert data into a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def plot_correlation_matrix(correlation_matrix):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of the correlation matrix.\n",
    "\n",
    "    Parameters:\n",
    "        correlation_matrix (pandas.DataFrame): The correlation matrix to plot.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2057f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate correlated dataframe\n",
    "df_correlated_data = generate_correlated_dataframe()\n",
    "\n",
    "# print correlated dataframe\n",
    "print(df_correlated_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf556da6",
   "metadata": {},
   "source": [
    "Using this data `df_correlated_data`, and a plotting function `plot_correlation_matrix` already made for you to use, letâ€™s look at the correlation matrix.\n",
    "\n",
    "Compute the correlation matrix from the dataset, then plot the results to the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b7fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "correlation_matrix = df_correlated_data.corr()\n",
    "\n",
    "# Plot the corrrelation matrix.\n",
    "plot_correlation_matrix(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ba0f21",
   "metadata": {},
   "source": [
    "* Red - High or Positive correlation.\n",
    "* Blue - Low or Negative correlation.\n",
    "\n",
    "What two variables are highly correlated here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d717cd30",
   "metadata": {},
   "source": [
    "Let's now do the same for our dataframe that we have been working on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec6977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "our_dataset_correlation_matrix = df.corr()\n",
    "\n",
    "# Plot the corrrelation matrix.\n",
    "plot_correlation_matrix(our_dataset_correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a82d41",
   "metadata": {},
   "source": [
    "What highly correlated variable could we remove here without losing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aaa2c2",
   "metadata": {},
   "source": [
    "<b>Practical Task 2.3</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a79f6",
   "metadata": {},
   "source": [
    "Using our dataset\n",
    "* First remove the highly correlated variable from the dataset. <br>\n",
    "\n",
    "* Then using the documentation here on [`One Hot Encoding`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), One-Hot encode  the `diabetes` column, using the `drop='first'` argument. \n",
    "\n",
    "> Note: You will need to import `OneHotEncoder` from the `sklearn` library as we have not used this in our notebook.\n",
    "\n",
    "* Plot a correlation matrix to check the results of both the previous points. i.e.: have you successfully removed the columns with high multicollinearity!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - remove the highly correlated variable\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa02b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - use one hot encoding from sklearn, to encode the diabetes column using the drop=First argument.\n",
    "######################################\n",
    "# Remember the drop='first' argument\n",
    "######################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c837177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - plot a new correlation matrix of dataset using function: plot_correlation_matrix()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe183e5",
   "metadata": {},
   "source": [
    "## Scale & Transform\n",
    "So far, we have looked at the categorical variables in our dataset and how to prepare them for machine learning. However, the numerical features also need to be reviewed before creating a model, this is what scaling and transforming is about. <br>\n",
    "\n",
    "Scaling is the process of adjusting the range of values or doing so through a mathematical operation or function applied to the data to achieve the desired scaling effect. Whilst transformation is the process of changing the distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d675bf",
   "metadata": {},
   "source": [
    "### Normalising or Scaling the Data\n",
    "Normalising data involves scaling numerical features to a standard range, typically between 0 and 1 or -1 and 1, to ensure consistency and comparability across variables. <br>This process is essential in data preprocessing to prevent features with larger scales from dominating those with smaller scales, which can adversely affect the performance of machine learning algorithms, especially those sensitive to the scale of features (e.g., K-means clustering, gradient descent-based algorithms).\n",
    "\n",
    "To implement Min-Max Scaling on our numerical features we need to import the following library.\n",
    "> Note: This process can also be applied to target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08db3a02",
   "metadata": {},
   "source": [
    "```python\n",
    "# import MinMaxScaler from sklearn.preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf501f8d",
   "metadata": {},
   "source": [
    "We are going to look at a simple dataset to demonstrate this before applying to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Dataframe\n",
    "data = {'Feature1': [10, 20, 30, 40, 50],\n",
    "        'Feature2': [100, 200, 300, 400, 500],\n",
    "        'Feature3': [150, 250, 350, 450, 550]}\n",
    "\n",
    "df_sample = pd.DataFrame(data)\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea8ebe0",
   "metadata": {},
   "source": [
    "Firstly, instantiate the MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b4acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152dd9ba",
   "metadata": {},
   "source": [
    "Apply to the simple dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ff4c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the data\n",
    "df_normalised_minmaxscaler = pd.DataFrame(minmax_scaler.fit_transform(df_sample), columns=df_sample.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5cf29f",
   "metadata": {},
   "source": [
    "And compare the output to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d013fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Dataframe:\")\n",
    "print(df_sample)\n",
    "print(\"\\nNormalised Dataframe:\")\n",
    "print(df_normalised_minmaxscaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9922beb",
   "metadata": {},
   "source": [
    "Another method is Z-score standardisation. This will scale the data so that it has a mean of 0 and standard deviation of 1. <br>\n",
    "To implement the Min-Max Scaling on our numerical features we need to import the following library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef6a08e",
   "metadata": {},
   "source": [
    "```python\n",
    "# import StandardScaler from sklearn.preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7887c0a3",
   "metadata": {},
   "source": [
    "Using the same small dataset of `df_sample`, set-up the StandardScaler, apply to the dataframe and print the results with the original to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd24713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StandardScaler\n",
    "zscore_scaler = StandardScaler()\n",
    "\n",
    "# Normalise the data\n",
    "df_normalised_standardscaler = pd.DataFrame(zscore_scaler.fit_transform(df_sample),\n",
    "                                            columns=df_sample.columns)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df_sample)\n",
    "print(\"\\nNormalised DataFrame:\")\n",
    "print(df_normalised_standardscaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9143edbe",
   "metadata": {},
   "source": [
    "The choice between Min-Max Scaling and Z-score Standardisation depends on the characteristics of your data and the requirements of your machine learning model. <br>\n",
    "Min-Max Scaling is more suitable for non-Gaussian (non-normal) distributed data or when you need data within a specific range, while Z-score Standardisation is preferable for Gaussian (normal) distributed data or when preserving the shape of the distribution is important, especially for algorithms that rely on distance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a126a",
   "metadata": {},
   "source": [
    "<b>Practical Task 2.4</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c119dd97",
   "metadata": {},
   "source": [
    "Using our dataset apply either the Min-Max scaler or the Z-score/Standard Scaler to the following numerical features.\n",
    "* Column: `cholesterol`.\n",
    "* Column: `bmi`.\n",
    "* Column: `cigarettes_per_day`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8d5366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609b3e27",
   "metadata": {},
   "source": [
    "### Transformation\n",
    "While normalisation rescales the data within new limits to reduce the impact of magnitude in the variance, transformation of features and/or target variables is a more radical technique. <br> \n",
    "Transformation changes the shape of the distribution such that the transformed data can be represented by a normal or approximate normal distribution. <br>\n",
    "\n",
    "Transforming data to approximate a normal distribution is crucial in machine learning for ensuring that the data meets the assumptions of many algorithms, such as linear regression and statistical tests. Normalising the distribution improves model stability, convergence, and performance. By stabilising variance and reducing skewness, transformed data facilitates better understanding of underlying relationships and leads to more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44616163",
   "metadata": {},
   "source": [
    "We will be working on our dataset for this section and will be plotting some distributions to show how we can transform them. <br>\n",
    "Generate this function, which will plot the distribution of one or two specified columns in the dataframe in two separate plots for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot transformation function\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_transformation(df, column1, column2=None):\n",
    "    \"\"\"\n",
    "    Plot the distribution of one or two specified columns in the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): DataFrame containing the data.\n",
    "        column1 (str): Name of the first column to be plotted.\n",
    "        column2 (str, optional): Name of the second column to be plotted. Defaults to None.\n",
    "    \"\"\"\n",
    "    # Set the style of seaborn\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Plot the first histogram\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(data=df, x=column1, kde=True)\n",
    "    plt.title(f'Distribution of {column1}')\n",
    "    plt.xlabel(f'{column1} (mmHg)')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Plot the second histogram if column2 is provided\n",
    "    if column2:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.histplot(data=df, x=column2, kde=True)\n",
    "        plt.title(f'Distribution of {column2}')\n",
    "        plt.xlabel(f'{column2} (mmHg)')\n",
    "        plt.ylabel('Frequency')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece2dc54",
   "metadata": {},
   "source": [
    "Use the `plot_transformation` function by passing in the name of our dataframe and the column name: `'systolic_bp'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104ce0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_transformation(df,'systolic_bp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a177a4fc",
   "metadata": {},
   "source": [
    "#### Power Transformer\n",
    "\n",
    "We are going to look at the PowerTransformer from `sklearn`. <br>\n",
    "The library for this has already been imported:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de2c60",
   "metadata": {},
   "source": [
    "```python\n",
    "# import PowerTransformer from sklearn.preprocessing\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c5b300",
   "metadata": {},
   "source": [
    "We are going to instantiate the `PowerTransformer` and apply it to the `systolic_bp` column, saving the results to a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef52dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PowerTransformer to systolic_bp\n",
    "power_transformer = PowerTransformer()\n",
    "df['systolic_bp_transformed_power'] = power_transformer.fit_transform(df[['systolic_bp']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436596a4",
   "metadata": {},
   "source": [
    "Let's look at some of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b51df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display the transformed DataFrame\n",
    "print(df[['systolic_bp', 'systolic_bp_transformed_power']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38337372",
   "metadata": {},
   "source": [
    "And now plot the before and after using the `plot_transformation` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2f6aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot before and after transformation\n",
    "plot_transformation(df,'systolic_bp','systolic_bp_transformed_power')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a1869b",
   "metadata": {},
   "source": [
    "#### Quantile Transformer\n",
    "\n",
    "Another transformer that we will look at now is the Quantile Transformer. <br>\n",
    "The library for this has also already been imported:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc21ebb",
   "metadata": {},
   "source": [
    "```python\n",
    "# import QuantileTransformer from sklearn.preprocessing\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14894201",
   "metadata": {},
   "source": [
    "The same as before, we instantiate the transformer - `QuantileTransformer` and apply it to the `systolic_bp` column. This time saving the results to another new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82b801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply QuantileTransformer to systolic_bp\n",
    "quantile_transformer = QuantileTransformer(output_distribution='normal')\n",
    "df['systolic_bp_transformed'] = quantile_transformer.fit_transform(df[['systolic_bp']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164cfc02",
   "metadata": {},
   "source": [
    "You will get a warning here: <br>\n",
    "`UserWarning: n_quantiles (1000) is greater than the total number of samples (100). n_quantiles is set to n_samples.`<br>\n",
    "Use this link to the `sklearn` documentation on [QuantileTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html#sklearn.preprocessing.QuantileTransformer) to see if you can fix the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5838fe9",
   "metadata": {},
   "source": [
    "Some of the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a258a9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the transformed DataFrame\n",
    "print(df[['systolic_bp', 'systolic_bp_transformed']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e890894f",
   "metadata": {},
   "source": [
    "Plot the before and after..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab68645",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot before and after transformation\n",
    "plot_transformation(df,'systolic_bp','systolic_bp_transformed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a4495",
   "metadata": {},
   "source": [
    "<b>Practical Task 2.5</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385fbfa1",
   "metadata": {},
   "source": [
    "With our dataset, use the `diastolic_bp` to:\n",
    "* Plot the distribution of 'diastolic_bp'. For this you can use the function `plot_transformation` for this.\n",
    "* Apply one of the transform methods to a new column `diastolic_bp_transformed`.\n",
    "* Plot the transformed distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b936aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f613573",
   "metadata": {},
   "source": [
    "Further reading on types of distribution transforms can be found [here](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_map_data_to_normal.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71a8a75",
   "metadata": {},
   "source": [
    "### Chapter Summary\n",
    "Well done on reaching the end of this chapter! <br>\n",
    "You should now be familiar with the following when it comes to feature engineering, scaling, and transforming a dataset:\n",
    "* Creating new features in the dataset.\n",
    "* Encoding categorical variables of different types and know when to use them.\n",
    "* Remove multicollinearity.\n",
    "* Scaling the features and target variables.\n",
    "* Transforming the distribution of features and target variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
